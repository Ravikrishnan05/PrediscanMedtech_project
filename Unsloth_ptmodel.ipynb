{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravikrishnan05/PrediscanMedtech_project/blob/main/Unsloth_ptmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4A4G9LfFoEEg"
      },
      "outputs": [],
      "source": [
        "# To run this, press \"Runtime\" and press \"Run all\" on a free Tesla T4 Google Colab instance!\n",
        "\n",
        "#    Join Discord if you need help + ⭐ Star us on Github ⭐\n",
        "# To install Unsloth on your own computer, follow the installation instructions on our Github page here.\n",
        "\n",
        "# You will learn how to do data prep, how to train, how to run the model, & how to save it\n",
        "\n",
        "# News\n",
        "# Unsloth now supports Text-to-Speech (TTS) models. Read our guide here.\n",
        "\n",
        "# Read our Qwen3 Guide and check out our new Dynamic 2.0 quants which outperforms other quantization methods!\n",
        "\n",
        "# Visit our docs for all our model uploads and notebooks.\n",
        "\n",
        "# To run this, press \"Runtime\" and press \"Run all\" on a free Tesla T4 Google Colab instance!\n",
        "# %%capture # Use %%capture to hide pip outputs if desired\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    print(\"Installing Unsloth for local environment...\")\n",
        "    !pip install \"unsloth[colab-new]@git+https://github.com/unslothai/unsloth.git\"\n",
        "else:\n",
        "    print(\"Installing Unsloth for Colab environment...\")\n",
        "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "    !pip install --no-deps \"unsloth[colab-new]@git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Cell 0.2: Additional Library Installations\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\nInstalling additional libraries for data processing and DICOM handling...\")\n",
        "!pip install -q pydicom pandas opencv-python Pillow scikit-learn matplotlib seaborn \"huggingface_hub>=0.23.0\" \"hf_transfer>=0.1.6\" \"datasets>=2.16.0\" sentencepiece protobuf\n",
        "\n",
        "# Install unsloth_zoo\n",
        "print(\"\\nInstalling unsloth_zoo...\")\n",
        "!pip install unsloth_zoo"
      ],
      "metadata": {
        "id": "fhJQhFboqc1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unsloth FastModel supports loading nearly any model now! This includes Vision and Text models!\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Cell 0.3: Unsloth Model Loading\n",
        "# -----------------------------------------------------------------------------\n",
        "from unsloth import FastLanguageModel # Changed from FastModel to FastLanguageModel as per recent Unsloth examples for language models\n",
        "import torch"
      ],
      "metadata": {
        "id": "aTMppuNfrAxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: MODEL SELECTION FOR YOUR TASK\n",
        "# The model \"unsloth/gemma-3-4b-it\" is a TEXT-BASED instruct model.\n",
        "# Your original code used MedGemma, a VISION-LANGUAGE model, and processed images.\n",
        "# If your task involves processing images to predict LDL, you MUST select a vision-language model.\n",
        "# Examples:\n",
        "#   - Search for Unsloth-quantized vision models: https://huggingface.co/unsloth\n",
        "#   - Try loading a standard HF vision model (e.g., \"google/medgemma-4b-pt\", \"llava-hf/llava-1.5-7b-hf\", \"microsoft/phi-3-vision-128k-instruct\")\n",
        "#     FastLanguageModel might support them. If so, set `finetune_vision_layers = True` in the PEFT setup.\n",
        "# For this example, we'll use the text model from the Unsloth template.\n",
        "# You will need to adapt your data processing (especially image handling in the Dataset)\n",
        "# if you use a text model for a vision task, or change the model_name.\n",
        "\"\"\"\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# --- Model Selection ---\n",
        "# We are focusing on MedGemma for vision-based LDL prediction.\n",
        "selected_model_name = \"google/medgemma-4b-pt\"\n",
        "\n",
        "print(f\"Attempting to load model: {selected_model_name}\")\n",
        "# When loading a multimodal model like MedGemma, FastLanguageModel handles it.\n",
        "# The 'tokenizer' returned will be a multimodal processor (e.g., GemmaProcessor)\n",
        "# which contains both the image_processor and the text_tokenizer.\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=selected_model_name,\n",
        "    max_seq_length=2048,  # Max sequence length for the language model part (less critical for pure vision regression)\n",
        "    dtype=None,           # Autodetect\n",
        "    load_in_4bit=True,    # Enable 4-bit quantization for memory efficiency\n",
        "    # token = \"hf_...\",   # Use if the model is gated\n",
        ")\n",
        "print(f\"Model {selected_model_name} loaded successfully.\")\n",
        "print(f\"Tokenizer type: {type(tokenizer)}\")\n",
        "\n",
        "# --- Verify Image Processor and Get Vision Feature Dimension ---\n",
        "# For MedGemma, the tokenizer is a GemmaProcessor which should have an 'image_processor'\n",
        "if hasattr(tokenizer, 'image_processor') and tokenizer.image_processor is not None:\n",
        "    print(\"Image processor found in tokenizer.\")\n",
        "    # The vision tower configuration is part of the main model's config for MedGemma\n",
        "    if hasattr(model.config, 'vision_config'):\n",
        "        vision_config = model.config.vision_config\n",
        "        # The vision feature dimension is typically 'hidden_size' of the vision_config\n",
        "        # For SigLIP (MedGemma's vision tower), it's usually referred to as hidden_size.\n",
        "        vision_feature_dim = vision_config.hidden_size\n",
        "        print(f\"Detected vision feature dimension from model.config.vision_config: {vision_feature_dim}\")\n",
        "    else:\n",
        "        print(\"ERROR: model.config.vision_config not found. Cannot determine vision_feature_dim automatically.\")\n",
        "        # Fallback: Try to inspect the vision_tower directly if it exists on the base model\n",
        "        # This path might be needed if Unsloth wraps the model differently.\n",
        "        base_model_ref = model.model if hasattr(model, 'model') else model\n",
        "        if hasattr(base_model_ref, 'vision_tower') and hasattr(base_model_ref.vision_tower, 'config'):\n",
        "            vision_feature_dim = base_model_ref.vision_tower.config.hidden_size\n",
        "            print(f\"Detected vision feature dimension from base_model.vision_tower.config: {vision_feature_dim}\")\n",
        "        else:\n",
        "            vision_feature_dim = None\n",
        "            print(\"ERROR: Could not access vision_tower.config. Manually inspect 'model' object and set vision_feature_dim.\")\n",
        "            print(\"Model structure:\", model) # Helps in debugging\n",
        "else:\n",
        "    print(\"ERROR: No image_processor found in the tokenizer. This is unexpected for MedGemma.\")\n",
        "    vision_feature_dim = None\n",
        "\n",
        "if vision_feature_dim is None:\n",
        "    print(\"CRITICAL ERROR: vision_feature_dim could not be determined. Regression head cannot be initialized correctly.\")\n",
        "    # You might need to manually set it based on MedGemma's architecture if auto-detection fails.\n",
        "    # For medgemma-4b-pt, the vision feature dimension (SigLIP-L/16) is 1024.\n",
        "    vision_feature_dim = 1152 # Example: Manually set if necessary\n",
        "    print(f\"Attempting to use manually set vision_feature_dim: {vision_feature_dim}\")\n",
        "\n",
        "\"\"\"\n",
        "# Note: For vision models, the 'tokenizer' might be a composite object\n",
        "# or you might access an image processor via `model.processor` or `tokenizer.image_processor`.\n",
        "# This depends on how Unsloth handles vision models.\n",
        "\n",
        "# Cell 0.3: Unsloth Model Loading (REVISED FOR FLOAT16)\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "selected_model_name = \"google/medgemma-4b-pt\"\n",
        "\n",
        "print(f\"Attempting to load model: {selected_model_name} with dtype=torch.float16\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=selected_model_name,\n",
        "    max_seq_length=2048,\n",
        "    dtype=torch.float16,  # <--- CRITICAL CHANGE HERE\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "print(f\"Model {selected_model_name} loaded successfully.\")\n",
        "print(f\"Tokenizer type: {type(tokenizer)}\")\n",
        "print(f\"Base model dtype after loading: {model.dtype}\") # Should now be torch.float16\n",
        "\n",
        "# --- Verify Image Processor and Get Vision Feature Dimension ---\n",
        "# (This part remains the same as your last working version of Cell 0.3)\n",
        "if hasattr(tokenizer, 'image_processor') and tokenizer.image_processor is not None:\n",
        "    print(\"Image processor found in tokenizer.\")\n",
        "    if hasattr(model.config, 'vision_config'):\n",
        "        vision_config = model.config.vision_config\n",
        "        vision_feature_dim = vision_config.hidden_size\n",
        "        print(f\"Detected vision feature dimension from model.config.vision_config: {vision_feature_dim}\")\n",
        "    else:\n",
        "        print(\"ERROR: model.config.vision_config not found. Cannot determine vision_feature_dim automatically.\")\n",
        "        base_model_ref = model.model if hasattr(model, 'model') else model\n",
        "        if hasattr(base_model_ref, 'vision_tower') and hasattr(base_model_ref.vision_tower, 'config'):\n",
        "            vision_feature_dim = base_model_ref.vision_tower.config.hidden_size\n",
        "            print(f\"Detected vision feature dimension from base_model.vision_tower.config: {vision_feature_dim}\")\n",
        "        else:\n",
        "            vision_feature_dim = None\n",
        "            print(\"ERROR: Could not access vision_tower.config. Manually inspect 'model' object and set vision_feature_dim.\")\n",
        "            # For medgemma-4b-pt, vision_feature_dim is 1024.\n",
        "            if vision_feature_dim is None:\n",
        "                print(\"Attempting to manually set vision_feature_dim to 1024 for MedGemma.\")\n",
        "                vision_feature_dim = 1024\n",
        "else:\n",
        "    print(\"ERROR: No image_processor found in the tokenizer. This is unexpected for MedGemma.\")\n",
        "    vision_feature_dim = 1024 # Fallback\n",
        "    print(f\"Attempting to manually set vision_feature_dim to {vision_feature_dim} due to missing image_processor.\")\n",
        "\n",
        "\n",
        "if vision_feature_dim is None:\n",
        "    print(\"CRITICAL ERROR: vision_feature_dim could not be determined. Regression head cannot be initialized correctly.\")"
      ],
      "metadata": {
        "id": "6CH2bYyCacJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After loading the model with Unsloth:\n",
        "# The actual path might be model.model.vision_tower if PEFT wraps it further\n",
        "base_medgemma_model = model.model if hasattr(model, 'model') else model # Access base model if PEFT wrapped\n",
        "\n",
        "if hasattr(base_medgemma_model, 'vision_tower') and hasattr(base_medgemma_model.vision_tower, 'config'):\n",
        "    vision_config = base_medgemma_model.vision_tower.config\n",
        "    vision_feature_dim = vision_config.hidden_size\n",
        "    print(f\"Detected vision feature dimension: {vision_feature_dim}\")\n",
        "    # Now define your regression head separately or as part of a wrapper\n",
        "    # regression_head = torch.nn.Linear(vision_feature_dim, 1)\n",
        "else:\n",
        "    print(\"ERROR: Could not access model.vision_tower.config to get vision_feature_dim.\")\n",
        "    print(\"Please inspect the 'model' object structure from Unsloth carefully.\")\n",
        "    # You might need to print(model) and explore its attributes\n",
        "    vision_feature_dim = None # Fallback"
      ],
      "metadata": {
        "id": "G7rHCq-aceho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Applying PEFT (LoRA) ---\")\n",
        "# `model` is the Unsloth-loaded MedGemma model from the previous cell.\n",
        "# We use get_peft_model for LoRA.\n",
        "RANDOM_SEED=42\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # LoRA rank (higher can mean more expressiveness but more params)\n",
        "    lora_alpha=32,  # LoRA alpha (scaling factor, often 2*r)\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\", # Recommended by Unsloth\n",
        "    random_state=RANDOM_SEED,\n",
        "    target_modules=None, # Let Unsloth automatically find layers for LoRA.\n",
        "                         # It should target both vision and language linear layers by default.\n",
        "    finetune_vision_layers=True, # CRITICAL: Ensure vision tower layers are targeted for LoRA\n",
        "    finetune_language_layers=False # OPTIONAL: For pure vision regression, we might not need to tune language layers.\n",
        "                                  # Set to False if language model outputs are not used by the regression head.\n",
        "                                  # If True (default), language LoRA adapters will also be trained.\n",
        ")\n",
        "print(\"PEFT (LoRA) adapters added to the MedGemma model.\")\n",
        "print(\"Trainable parameters after LoRA:\")\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "APwpZTzwUoES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import torch.nn as nn\n",
        "\n",
        "class MedGemmaVisionRegressor(nn.Module):\n",
        "    def __init__(self, peft_medgemma_model, vision_feature_dim_input: int):\n",
        "        super().__init__()\n",
        "        self.medgemma_model = peft_medgemma_model # This is the PEFT-adapted model from Unsloth\n",
        "\n",
        "        # The regression head takes the pooled vision features and outputs 1 LDL value\n",
        "        self.regression_head = nn.Sequential(\n",
        "            nn.Linear(vision_feature_dim_input, vision_feature_dim_input // 2), # Intermediate layer\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(vision_feature_dim_input // 2, 1) # Output layer\n",
        "        )\n",
        "\n",
        "        # Note: Freezing of base MedGemma layers is handled by Unsloth's PEFT.\n",
        "        # LoRA adapters are trainable. The regression_head is also trainable.\n",
        "\n",
        "    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
        "        # The peft_medgemma_model is already a PeftModel.\n",
        "        # We need to pass pixel_values to it.\n",
        "        # The MedGemma model's forward pass can take pixel_values directly.\n",
        "        # It will internally use its vision_tower.\n",
        "        # For regression from vision, we typically want the pooled image features.\n",
        "\n",
        "        # Option 1: If the PEFT model directly gives vision features or allows access\n",
        "        # The `Gemma3ForMultiModalGeneration` (base for MedGemma) has `vision_tower`\n",
        "        # and can output `image_embeds` or similar.\n",
        "        # When using PEFT, the base model is often accessed via `self.medgemma_model.model`\n",
        "\n",
        "        base_model = self.medgemma_model.model # Access the original model underlying PEFT\n",
        "\n",
        "        # Get vision embeddings from the vision_tower\n",
        "        # The vision_tower (SigLIP) in MedGemma outputs pooled features.\n",
        "        vision_outputs = base_model.vision_tower(pixel_values=pixel_values, return_dict=True)\n",
        "\n",
        "        # `pooler_output` from SigLipVisionModelOutput is [batch_size, vision_feature_dim]\n",
        "        image_features = vision_outputs.pooler_output\n",
        "\n",
        "        if image_features is None:\n",
        "            # Fallback if pooler_output is not directly available (should be for SigLIP)\n",
        "            # This might happen if the model structure is different than expected.\n",
        "            # For ViT-like models, the first token's embedding ([CLS] token) is often used.\n",
        "            if hasattr(vision_outputs, 'last_hidden_state'):\n",
        "                image_features = vision_outputs.last_hidden_state[:, 0, :] # CLS token embedding\n",
        "            else:\n",
        "                raise ValueError(\"Could not extract pooled image features (pooler_output or CLS token) from vision_tower output.\")\n",
        "\n",
        "        # Pass vision features through the regression head\n",
        "        ldl_prediction = self.regression_head(image_features)\n",
        "        return ldl_prediction\n",
        "\n",
        "# --- Instantiate the Regressor Model ---\n",
        "if vision_feature_dim is not None:\n",
        "    # `model` here is the PEFT-adapted MedGemma model from Cell 0.4\n",
        "    regressor_model = MedGemmaVisionRegressor(model, vision_feature_dim)\n",
        "    print(f\"MedGemmaVisionRegressor created with regression head input dim {vision_feature_dim}.\")\n",
        "\n",
        "    # Move to GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    regressor_model.to(device)\n",
        "    print(f\"Regressor model moved to {device}.\")\n",
        "\n",
        "    print(\"\\nTrainable parameters of the Regressor Model (includes LoRA + head):\")\n",
        "    total_params = 0\n",
        "    trainable_params = 0\n",
        "    for name, param in regressor_model.named_parameters():\n",
        "        total_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "            # print(f\"Trainable: {name}, Shape: {param.shape}\") # Uncomment to see all trainable params\n",
        "    print(f\"Total parameters in RegressorModel: {total_params:,}\")\n",
        "    print(f\"Trainable parameters in RegressorModel: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "\n",
        "else:\n",
        "    regressor_model = None\n",
        "    print(\"CRITICAL ERROR: Cannot create MedGemmaVisionRegressor because vision_feature_dim is None.\")\n",
        "\n",
        "\"\"\"\n",
        "# Cell 0.5: Define and Instantiate Custom Model Wrapper (MedGemmaVisionRegressor) - REVISED FOR FLOAT16\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class MedGemmaVisionRegressor(nn.Module):\n",
        "    def __init__(self, peft_medgemma_model, vision_feature_dim_input: int):\n",
        "        super().__init__()\n",
        "        self.medgemma_model = peft_medgemma_model\n",
        "        self.target_dtype = self.medgemma_model.dtype # Should be torch.float16 now\n",
        "        print(f\"[Regressor Init] Base PEFT model target dtype: {self.target_dtype}\")\n",
        "\n",
        "        self.regression_head = nn.Sequential(\n",
        "            nn.Linear(vision_feature_dim_input, vision_feature_dim_input // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(vision_feature_dim_input // 2, 1)\n",
        "        )\n",
        "\n",
        "        if self.target_dtype == torch.float16: # Explicitly check for float16\n",
        "            print(f\"[Regressor Init] Casting regression_head to {self.target_dtype}.\")\n",
        "            self.regression_head = self.regression_head.to(dtype=self.target_dtype)\n",
        "        elif self.target_dtype is not None: # If it's something else, print a warning but still cast\n",
        "            print(f\"[Regressor Init] WARNING: Base model dtype is {self.target_dtype}, not float16. Casting regression_head to {self.target_dtype} anyway.\")\n",
        "            self.regression_head = self.regression_head.to(dtype=self.target_dtype)\n",
        "\n",
        "\n",
        "    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
        "        # Input pixel_values should be cast to self.target_dtype (float16) in the training loop\n",
        "\n",
        "        base_model = self.medgemma_model.model\n",
        "        vision_outputs = base_model.vision_tower(pixel_values=pixel_values, return_dict=True)\n",
        "        image_features = vision_outputs.pooler_output\n",
        "\n",
        "        if image_features is None:\n",
        "            if hasattr(vision_outputs, 'last_hidden_state'):\n",
        "                image_features = vision_outputs.last_hidden_state[:, 0, :]\n",
        "            else:\n",
        "                raise ValueError(\"Could not extract pooled image features from vision_tower output.\")\n",
        "\n",
        "        # Ensure image_features are in float16 before feeding to regression_head\n",
        "        if image_features.dtype != self.target_dtype:\n",
        "            image_features = image_features.to(self.target_dtype)\n",
        "\n",
        "        ldl_prediction = self.regression_head(image_features)\n",
        "        return ldl_prediction\n",
        "\n",
        "# --- Instantiate the Regressor Model ---\n",
        "if vision_feature_dim is not None and 'model' in locals() and model is not None:\n",
        "    regressor_model = MedGemmaVisionRegressor(model, vision_feature_dim) # `model` is from Cell 0.3\n",
        "    print(f\"MedGemmaVisionRegressor created with regression head input dim {vision_feature_dim}.\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    regressor_model.to(device)\n",
        "    print(f\"Regressor model moved to {device}.\")\n",
        "\n",
        "    # Parameter printing (same as before)\n",
        "    print(\"\\nTrainable parameters of the Regressor Model (includes LoRA + head):\")\n",
        "    total_params = 0; trainable_params = 0\n",
        "    for name, param in regressor_model.named_parameters():\n",
        "        total_params += param.numel()\n",
        "        if param.requires_grad: trainable_params += param.numel()\n",
        "    print(f\"Total parameters in RegressorModel: {total_params:,}\")\n",
        "    print(f\"Trainable parameters in RegressorModel: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "else:\n",
        "    regressor_model = None\n",
        "    print(\"CRITICAL ERROR: Cannot create MedGemmaVisionRegressor. Check 'vision_feature_dim' and 'model'.\")"
      ],
      "metadata": {
        "id": "ZKo4mLwBchne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Cell 1: PyTorch/HuggingFace Imports and Setup (Adapted from user's Cell 1)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\nImporting libraries...\")\n",
        "# Python Standard Libraries\n",
        "import shutil # os, zipfile already imported or not needed here\n",
        "import zipfile\n",
        "\n",
        "# Third-party Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pydicom\n",
        "import cv2 # OpenCV\n",
        "from PIL import Image\n",
        "\n",
        "# PyTorch\n",
        "# import torch # Already imported\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Hugging Face (tokenizer is already loaded by Unsloth)\n",
        "# from transformers import AutoProcessor # Replaced by Unsloth's tokenizer\n",
        "\n",
        "# Plotting (optional, but often useful)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Colab specific\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"--- Library Version Checks ---\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "import sklearn\n",
        "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
        "# print(f\"TensorFlow Version: {tf.__version__}\") # TensorFlow not used in this Unsloth/PyTorch setup\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"PyTorch CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU available for PyTorch: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"GPU not available for PyTorch, using CPU.\")\n",
        "\n",
        "# For reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "print(\"\\nCell 1: Imports and basic setup complete.\")"
      ],
      "metadata": {
        "id": "BUr3OCOxsODW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# Cell 2: Configuration and Unzip Data (From user's Cell 2)\n",
        "# --------------------------------------------------\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# --- Configuration ---\n",
        "DRIVE_CSV_PATH = \"/content/drive/MyDrive/cp.csv\"\n",
        "DRIVE_ZIP_PATH = \"/content/drive/MyDrive/1000-20250517T062750Z-1-001.zip\" # Your image ZIP on Drive\n",
        "\n",
        "LOCAL_EXTRACT_PATH = \"/content/medgemma_extracted_images\"\n",
        "LOCAL_IMAGES_ROOT = os.path.join(LOCAL_EXTRACT_PATH, \"1000\") # Adjusted to match your structure\n",
        "LOCAL_CSV_PATH = \"/content/medgemma_cp.csv\"\n",
        "\n",
        "# --- Unzip Data (if not already done or if re-running) ---\n",
        "if os.path.exists(DRIVE_CSV_PATH):\n",
        "    shutil.copy(DRIVE_CSV_PATH, LOCAL_CSV_PATH)\n",
        "    print(f\"CSV copied to {LOCAL_CSV_PATH}\")\n",
        "else:\n",
        "    print(f\"ERROR: CSV file not found at {DRIVE_CSV_PATH}\")\n",
        "\n",
        "if os.path.exists(LOCAL_EXTRACT_PATH):\n",
        "    print(f\"Removing existing extraction directory: {LOCAL_EXTRACT_PATH}\")\n",
        "    shutil.rmtree(LOCAL_EXTRACT_PATH)\n",
        "os.makedirs(LOCAL_EXTRACT_PATH, exist_ok=True)\n",
        "print(f\"Created local extraction directory: {LOCAL_EXTRACT_PATH}\")\n",
        "\n",
        "if os.path.exists(DRIVE_ZIP_PATH):\n",
        "    print(f\"Unzipping {DRIVE_ZIP_PATH} to {LOCAL_EXTRACT_PATH}...\")\n",
        "    with zipfile.ZipFile(DRIVE_ZIP_PATH, 'r') as zip_ref:\n",
        "        zip_ref.extractall(LOCAL_EXTRACT_PATH)\n",
        "    print(\"Unzipping complete.\")\n",
        "    if os.path.exists(LOCAL_IMAGES_ROOT):\n",
        "        print(f\"Image root folder found at: {LOCAL_IMAGES_ROOT}\")\n",
        "    else:\n",
        "        print(f\"ERROR: Expected image root folder '{LOCAL_IMAGES_ROOT}' not found after unzipping. Check ZIP structure.\")\n",
        "        print(f\"Contents of {LOCAL_EXTRACT_PATH}: {os.listdir(LOCAL_EXTRACT_PATH)}\")\n",
        "\n",
        "else:\n",
        "    print(f\"ERROR: ZIP file not found at {DRIVE_ZIP_PATH}\")\n",
        "\n",
        "print(\"\\nCell 2: Data unzipping complete.\")\n"
      ],
      "metadata": {
        "id": "qKhbFStitDQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# Cell 3: Load and Filter Clinical Data to create image_df (From user's Cell 3)\n",
        "# --------------------------------------------------\n",
        "image_df = pd.DataFrame()\n",
        "\n",
        "if not os.path.exists(LOCAL_CSV_PATH):\n",
        "    print(f\"FATAL ERROR: Clinical CSV file not found at the expected local path: {LOCAL_CSV_PATH}\")\n",
        "else:\n",
        "    df_raw_from_cell3 = pd.read_csv(LOCAL_CSV_PATH)\n",
        "    print(f\"Initial number of rows in clinical data (Cell 3): {len(df_raw_from_cell3)}\")\n",
        "\n",
        "    person_id_col_name_c3 = 'person_id'\n",
        "    ldl_col_name_c3 = \"LDL Cholesterol Calculation (mg/dL)\" # Ensure this matches your CSV header\n",
        "\n",
        "    if not (person_id_col_name_c3 in df_raw_from_cell3.columns and ldl_col_name_c3 in df_raw_from_cell3.columns):\n",
        "        print(f\"ERROR: Required columns ('{person_id_col_name_c3}' or '{ldl_col_name_c3}') not found in CSV.\")\n",
        "        print(f\"Available columns: {df_raw_from_cell3.columns.tolist()}\")\n",
        "    else:\n",
        "        df_selected_c3 = df_raw_from_cell3[[person_id_col_name_c3, ldl_col_name_c3]].copy()\n",
        "        df_selected_c3.rename(columns={ldl_col_name_c3: 'LDL_temp'}, inplace=True)\n",
        "        df_selected_c3['LDL_temp'] = pd.to_numeric(df_selected_c3['LDL_temp'], errors='coerce')\n",
        "        df_selected_c3.dropna(subset=['LDL_temp'], inplace=True)\n",
        "        df_selected_c3 = df_selected_c3[df_selected_c3['LDL_temp'] > 0].copy()\n",
        "        df_selected_c3[person_id_col_name_c3] = df_selected_c3[person_id_col_name_c3].astype(str)\n",
        "        print(f\"Cleaned clinical data (positive LDLs only): {len(df_selected_c3)} records.\")\n",
        "\n",
        "        ldl_lookup_c3 = df_selected_c3.set_index(person_id_col_name_c3)['LDL_temp'].to_dict()\n",
        "\n",
        "        if not (os.path.exists(LOCAL_IMAGES_ROOT) and os.path.isdir(LOCAL_IMAGES_ROOT)):\n",
        "            print(f\"FATAL ERROR: Images root path '{LOCAL_IMAGES_ROOT}' does not exist or is not a directory.\")\n",
        "        else:\n",
        "            available_folders_c3 = set(os.listdir(LOCAL_IMAGES_ROOT))\n",
        "            valid_ids_clinical_c3 = set(ldl_lookup_c3.keys())\n",
        "            common_person_ids_c3 = sorted(list(valid_ids_clinical_c3 & available_folders_c3))\n",
        "            print(f\"Found {len(common_person_ids_c3)} common person_ids for mapping.\")\n",
        "\n",
        "            image_records_list = []\n",
        "            for pid_c3 in common_person_ids_c3:\n",
        "                folder_path_c3 = os.path.join(LOCAL_IMAGES_ROOT, pid_c3)\n",
        "                ldl_val_c3 = ldl_lookup_c3[pid_c3]\n",
        "                if os.path.isdir(folder_path_c3):\n",
        "                    for filename_c3 in os.listdir(folder_path_c3):\n",
        "                        if filename_c3.lower().endswith(\".dcm\"):\n",
        "                            image_path_c3 = os.path.join(folder_path_c3, filename_c3)\n",
        "                            image_records_list.append({\n",
        "                                \"person_id\": pid_c3,\n",
        "                                \"image_path\": image_path_c3,\n",
        "                                \"LDL\": ldl_val_c3\n",
        "                            })\n",
        "            image_df = pd.DataFrame(image_records_list)\n",
        "            if not image_df.empty:\n",
        "                print(f\"Final image_df created with {len(image_df)} image-LDL pairs.\")\n",
        "                from IPython.display import display # For better display in Colab\n",
        "                display(image_df.head())\n",
        "                print(f\"LDL stats in final image_df: min={image_df['LDL'].min()}, max={image_df['LDL'].max()}, mean={image_df['LDL'].mean()}\")\n",
        "            else:\n",
        "                print(\"WARNING: image_df is empty after mapping. Check paths, IDs, and DICOM file existence.\")\n",
        "print(\"\\nCell 3: image_df preparation complete.\")"
      ],
      "metadata": {
        "id": "nyRLqIrUtLLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Cell 4: Verify image_df (Adapted from user's Cell 4)\n",
        "# -----------------------------------------------------------------------------\n",
        "if 'image_df' in locals() and isinstance(image_df, pd.DataFrame) and not image_df.empty:\n",
        "    print(f\"\\nContinuing with 'image_df' which has {len(image_df)} records.\")\n",
        "    print(\"Columns in image_df:\", image_df.columns.tolist())\n",
        "    from IPython.display import display # Ensure display is imported\n",
        "    print(\"Sample of image_df:\")\n",
        "    display(image_df.head())\n",
        "\n",
        "    required_cols = ['person_id', 'image_path', 'LDL']\n",
        "    if not all(col in image_df.columns for col in required_cols):\n",
        "        print(f\"ERROR: 'image_df' is missing one or more required columns: {required_cols}. Please re-run Cell 3.\")\n",
        "    elif image_df['LDL'].min() <= 0:\n",
        "        print(f\"ERROR: 'image_df' still contains non-positive LDL values. LDL min: {image_df['LDL'].min()}. Please re-run filtering in Cell 3.\")\n",
        "    else:\n",
        "        print(\"'image_df' seems okay to proceed.\")\n",
        "else:\n",
        "    print(\"ERROR: 'image_df' not found or is empty. Please ensure Cell 3 (data preparation) has been run successfully.\")\n",
        "    # To prevent later errors, create an empty df if it's missing, though this indicates a problem.\n",
        "    if 'image_df' not in locals() or not isinstance(image_df, pd.DataFrame):\n",
        "        image_df = pd.DataFrame(columns=['person_id', 'image_path', 'LDL'])\n",
        "\n",
        "\n",
        "print(f\"\\nUsing Unsloth loaded model: {selected_model_name}\") # From Cell 0.3\n",
        "print(\"\\nCell 4: image_df verification and Model ID check complete.\")"
      ],
      "metadata": {
        "id": "jKystT4Bt0Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Cell 5: Unsloth Tokenizer/Processor Info (Adapted from user's Cell 5)\n",
        "# -----------------------------------------------------------------------------\n",
        "# The `medgemma_processor` is now replaced by the `tokenizer` from Unsloth.\n",
        "# For vision models, this tokenizer might wrap an image processor,\n",
        "# or `model.processor` might be set by Unsloth.\n",
        "\n",
        "# This cell's original purpose was to find TARGET_SIZE_MEDGEMMA.\n",
        "# For MedGemma, the image processor (part of the 'tokenizer' object) handles resizing.\n",
        "# We can inspect the image_processor's configuration.\n",
        "\n",
        "print(\"\\n--- Inspecting MedGemma Image Processor ---\")\n",
        "TARGET_SIZE_FOR_IMAGES = None # Will be determined by the image_processor\n",
        "\n",
        "if 'tokenizer' in locals() and hasattr(tokenizer, 'image_processor') and tokenizer.image_processor is not None:\n",
        "    medgemma_image_processor = tokenizer.image_processor\n",
        "    print(f\"MedGemma Image Processor Type: {type(medgemma_image_processor)}\")\n",
        "\n",
        "    # The image processor config usually has 'size' information.\n",
        "    # For SigLIPImageProcessor (used by MedGemma), it's often under `size` directly.\n",
        "    # The 'size' attribute can be an int (for shortest_edge) or a dict {'height': H, 'width': W}.\n",
        "    if hasattr(medgemma_image_processor, 'size'):\n",
        "        size_info = medgemma_image_processor.size\n",
        "        print(f\"  Image processor 'size' attribute: {size_info}\")\n",
        "        if isinstance(size_info, int): # e.g., size=224 means shortest edge is 224\n",
        "            # MedGemma models often use square inputs, e.g., 224x224 for SigLIP-B, 384x384 for SigLIP-L\n",
        "            # The MedGemma paper mentions images are resized to 896×896 for their experiments.\n",
        "            # However, the underlying SigLIP processor might have its own default.\n",
        "            # Let's check if 'crop_size' is also available, which is often the final input size.\n",
        "            if hasattr(medgemma_image_processor, 'crop_size') and medgemma_image_processor.crop_size is not None:\n",
        "                crop_info = medgemma_image_processor.crop_size\n",
        "                if isinstance(crop_info, int):\n",
        "                    TARGET_SIZE_FOR_IMAGES = (crop_info, crop_info)\n",
        "                elif isinstance(crop_info, dict) and 'height' in crop_info and 'width' in crop_info:\n",
        "                    TARGET_SIZE_FOR_IMAGES = (crop_info['height'], crop_info['width'])\n",
        "                print(f\"  Using 'crop_size' for TARGET_SIZE_FOR_IMAGES: {TARGET_SIZE_FOR_IMAGES}\")\n",
        "\n",
        "            if TARGET_SIZE_FOR_IMAGES is None: # If crop_size wasn't definitive\n",
        "                 # If size is int, assume square image based on that size for processing.\n",
        "                 # The processor itself will handle the exact resizing logic.\n",
        "                 # We use this for our basic transforms if the processor fails.\n",
        "                 TARGET_SIZE_FOR_IMAGES = (size_info, size_info)\n",
        "                 print(f\"  Using 'size' attribute for TARGET_SIZE_FOR_IMAGES (assuming square): {TARGET_SIZE_FOR_IMAGES}\")\n",
        "\n",
        "        elif isinstance(size_info, dict) and 'height' in size_info and 'width' in size_info:\n",
        "            TARGET_SIZE_FOR_IMAGES = (size_info['height'], size_info['width'])\n",
        "            print(f\"  Using 'size' dict for TARGET_SIZE_FOR_IMAGES: {TARGET_SIZE_FOR_IMAGES}\")\n",
        "        else:\n",
        "            print(\"  Could not determine target size from image_processor.size. Check processor config.\")\n",
        "    else:\n",
        "        print(\"  Image processor does not have a direct 'size' attribute. Check its config details.\")\n",
        "\n",
        "    # Fallback if still not found, to MedGemma paper's mentioned size\n",
        "    if TARGET_SIZE_FOR_IMAGES is None:\n",
        "        TARGET_SIZE_FOR_IMAGES = (896, 896) # Default from MedGemma paper if not found in processor\n",
        "        print(f\"  Falling back to default TARGET_SIZE_FOR_IMAGES: {TARGET_SIZE_FOR_IMAGES} (from MedGemma paper)\")\n",
        "else:\n",
        "    print(\"ERROR: MedGemma image_processor not found in tokenizer. Cannot determine target image size.\")\n",
        "    TARGET_SIZE_FOR_IMAGES = (896, 896) # Fallback\n",
        "    print(f\"  Using fallback TARGET_SIZE_FOR_IMAGES: {TARGET_SIZE_FOR_IMAGES}\")\n",
        "\n",
        "print(f\"Final TARGET_SIZE_FOR_IMAGES to be used by Dataset (if processor fails or for reference): {TARGET_SIZE_FOR_IMAGES}\")\n",
        "print(\"\\nCell 5: MedGemma image processor check complete.\")"
      ],
      "metadata": {
        "id": "KOsK3Cd4t4G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Cell 6: Data Splitting (Patient-Level) and LDL Normalization (From user's Cell 6)\n",
        "# -----------------------------------------------------------------------------\n",
        "train_df, val_df, test_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "ldl_scaler = None # Will store the fitted StandardScaler\n",
        "\n",
        "if 'image_df' in locals() and not image_df.empty:\n",
        "    print(f\"\\nStarting data splitting for {len(image_df)} image-LDL pairs...\")\n",
        "    if 'person_id' not in image_df.columns:\n",
        "        print(\"ERROR: 'person_id' column missing in image_df. Cannot perform patient-level split. Please check image_df preparation in Cell 3.\")\n",
        "    else:\n",
        "        unique_person_ids = image_df['person_id'].unique()\n",
        "        print(f\"Total unique patients for splitting: {len(unique_person_ids)}\")\n",
        "\n",
        "        if len(unique_person_ids) < 3: # Need at least 3 patients for train/val/test\n",
        "            print(\"Warning: Not enough unique patients for a robust 3-way (train/validation/test) split.\")\n",
        "            # Simplified split logic for few patients\n",
        "            if len(unique_person_ids) == 2:\n",
        "                train_pids, val_pids = train_test_split(unique_person_ids, test_size=0.5, random_state=RANDOM_SEED)\n",
        "                test_pids = np.array([]) # Empty array for consistency\n",
        "            elif len(unique_person_ids) == 1:\n",
        "                train_pids = unique_person_ids\n",
        "                val_pids, test_pids = np.array([]), np.array([])\n",
        "            else: # 0 patients\n",
        "                train_pids, val_pids, test_pids = np.array([]), np.array([]), np.array([])\n",
        "        else:\n",
        "            # Standard 70% train, 15% validation, 15% test split of person_ids\n",
        "            train_pids, temp_pids = train_test_split(\n",
        "                unique_person_ids, test_size=0.30, random_state=RANDOM_SEED # 70% train, 30% temp\n",
        "            )\n",
        "            if len(temp_pids) > 1 : # Ensure there's at least 2 for val/test split\n",
        "                 val_pids, test_pids = train_test_split(\n",
        "                    temp_pids, test_size=0.50, random_state=RANDOM_SEED # Split temp 50/50 for val/test (15% each of total)\n",
        "                )\n",
        "            elif len(temp_pids) == 1: # Only one patient left for temp\n",
        "                val_pids = temp_pids # Assign to validation\n",
        "                test_pids = np.array([])\n",
        "            else: # No patients left for temp\n",
        "                val_pids, test_pids = np.array([]), np.array([])\n",
        "\n",
        "\n",
        "        train_df = image_df[image_df['person_id'].isin(train_pids)].copy()\n",
        "        val_df = image_df[image_df['person_id'].isin(val_pids)].copy()\n",
        "        test_df = image_df[image_df['person_id'].isin(test_pids)].copy()\n",
        "\n",
        "        print(f\"Train set: {len(train_df)} samples from {len(train_pids)} patients.\")\n",
        "        print(f\"Validation set: {len(val_df)} samples from {len(val_pids)} patients.\")\n",
        "        print(f\"Test set: {len(test_df)} samples from {len(test_pids)} patients.\")\n",
        "\n",
        "        # Sanity check for patient overlap\n",
        "        if len(train_pids)>0 and len(val_pids)>0: assert len(set(train_pids) & set(val_pids)) == 0, \"Patient overlap train/val!\"\n",
        "        if len(train_pids)>0 and len(test_pids)>0: assert len(set(train_pids) & set(test_pids)) == 0, \"Patient overlap train/test!\"\n",
        "        if len(val_pids)>0 and len(test_pids)>0: assert len(set(val_pids) & set(test_pids)) == 0, \"Patient overlap val/test!\"\n",
        "        print(\"Patient-level splits verified (no overlap if sets are non-empty).\")\n",
        "\n",
        "        # --- LDL Value Normalization ---\n",
        "        if not train_df.empty and 'LDL' in train_df.columns:\n",
        "            print(\"\\nNormalizing LDL values using StandardScaler...\")\n",
        "            ldl_scaler = StandardScaler()\n",
        "            # Fit the scaler ONLY on the training data's LDL values\n",
        "            train_df['LDL_scaled'] = ldl_scaler.fit_transform(train_df[['LDL']])\n",
        "\n",
        "            # Transform validation and test data using the FITTED scaler\n",
        "            if not val_df.empty:\n",
        "                val_df['LDL_scaled'] = ldl_scaler.transform(val_df[['LDL']])\n",
        "            else: # Add LDL_scaled column even if empty, for consistency\n",
        "                val_df['LDL_scaled'] = pd.Series(dtype='float64')\n",
        "\n",
        "            if not test_df.empty:\n",
        "                test_df['LDL_scaled'] = ldl_scaler.transform(test_df[['LDL']])\n",
        "            else:\n",
        "                test_df['LDL_scaled'] = pd.Series(dtype='float64')\n",
        "\n",
        "            print(\"LDL normalization complete.\")\n",
        "            print(\"Scaled LDL stats in train_df (should be mean~0, std~1):\")\n",
        "            from IPython.display import display # Ensure display is imported\n",
        "            display(train_df['LDL_scaled'].describe())\n",
        "\n",
        "            # Optional: Save the scaler\n",
        "            # import joblib\n",
        "            # scaler_filename = 'ldl_scaler_medgemma.joblib'\n",
        "            # joblib.dump(ldl_scaler, scaler_filename)\n",
        "            # print(f\"LDL scaler saved to {scaler_filename}\")\n",
        "        else:\n",
        "            print(\"Train DataFrame is empty or 'LDL' column missing. Skipping LDL normalization.\")\n",
        "else:\n",
        "    print(\"image_df is empty (from Cell 3). Skipping data splitting and LDL normalization.\")\n",
        "\n",
        "print(\"\\nCell 6: Data splitting and LDL normalization attempt complete.\")"
      ],
      "metadata": {
        "id": "WkxW5lpBt-7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Cell 5.1 (from user, now Cell 6.1): Check Unsloth tokenizer/model.processor\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n--- Sanity Check for Unsloth Components (Cell 6.1) ---\")\n",
        "if 'tokenizer' in locals() and tokenizer is not None:\n",
        "    print(f\"Unsloth tokenizer IS LOADED. Type: {type(tokenizer)}\")\n",
        "    if hasattr(tokenizer, 'image_processor') and tokenizer.image_processor is not None:\n",
        "        print(f\"  It has a tokenizer.image_processor of type: {type(tokenizer.image_processor)}\")\n",
        "    else:\n",
        "        print(\"  It does NOT have a direct `tokenizer.image_processor` attribute (or it's None).\")\n",
        "\n",
        "    if hasattr(model, 'processor') and model.processor is not None:\n",
        "        print(f\"Unsloth model.processor IS LOADED. Type: {type(model.processor)}\")\n",
        "        if hasattr(model.processor, 'image_processor') and model.processor.image_processor is not None:\n",
        "             print(f\"  model.processor has an image_processor component of type: {type(model.processor.image_processor)}\")\n",
        "    else:\n",
        "        print(\"  The model does NOT have a `model.processor` attribute (or it's None).\")\n",
        "\n",
        "    if not (hasattr(tokenizer, 'image_processor') and tokenizer.image_processor is not None) and \\\n",
        "       not (hasattr(model, 'processor') and model.processor is not None and hasattr(model.processor, 'image_processor')):\n",
        "        print(f\"  WARNING: No obvious image processor found. The model '{selected_model_name}' may be text-only.\")\n",
        "        print(\"  If your task requires image input, ensure you've selected a vision-language model and that Unsloth loads its image processor correctly.\")\n",
        "else:\n",
        "    print(\"Unsloth tokenizer IS NOT LOADED or is None.\")"
      ],
      "metadata": {
        "id": "ExnNlqz_uEpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c19e4e06"
      },
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Cell 7: Custom PyTorch Dataset for DICOM Images and LDL (New Cell)\n",
        "# -----------------------------------------------------------------------------\n",
        "# Helper for printing messages only once during dataset iteration or training\n",
        "printed_messages_dataset = set()\n",
        "def print_once_dataset(message):\n",
        "    global printed_messages_dataset\n",
        "    if message not in printed_messages_dataset:\n",
        "        print(message)\n",
        "        printed_messages_dataset.add(message)\n",
        "\n",
        "import torchvision.transforms as T # Import T for transforms\n",
        "\n",
        "class MedGemmaVisionDataset(Dataset):\n",
        "    def __init__(self, dataframe, medgemma_tokenizer_processor, target_img_size_ref=(896, 896)):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataframe (pd.DataFrame): DataFrame with 'image_path' and 'LDL_scaled' columns.\n",
        "            medgemma_tokenizer_processor: The multimodal processor from Unsloth (contains image_processor).\n",
        "            target_img_size_ref (tuple): Reference target image size, primarily for fallback.\n",
        "                                         The image_processor itself determines the actual processing.\n",
        "        \"\"\"\n",
        "        self.dataframe = dataframe\n",
        "        self.processor = medgemma_tokenizer_processor # This is the GemmaProcessor (or similar)\n",
        "        self.target_size_ref = target_img_size_ref # For fallback basic transforms\n",
        "\n",
        "        if not hasattr(self.processor, 'image_processor') or self.processor.image_processor is None:\n",
        "            raise ValueError(\"The provided processor must have a valid 'image_processor' attribute for MedGemma.\")\n",
        "\n",
        "        # Basic image transforms (fallback if image_processor fails for an image)\n",
        "        self.basic_transforms = T.Compose([\n",
        "            T.Resize(self.target_size_ref),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Imagenet stats\n",
        "        ])\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def load_and_preprocess_dicom(self, dicom_path):\n",
        "        try:\n",
        "            dicom_file = pydicom.dcmread(dicom_path)\n",
        "            pixel_array = dicom_file.pixel_array\n",
        "\n",
        "            # Normalize pixel data to 0-255 and ensure 3 channels (RGB)\n",
        "            # This is a common pre-step before PIL conversion for many image processors\n",
        "            if pixel_array.dtype != np.uint8:\n",
        "                pixel_array = pixel_array.astype(np.float32)\n",
        "                min_val, max_val = np.min(pixel_array), np.max(pixel_array)\n",
        "                if max_val > min_val:\n",
        "                    pixel_array = (pixel_array - min_val) / (max_val - min_val) * 255.0\n",
        "                else: # Handle case where all pixels are the same\n",
        "                    pixel_array = np.zeros_like(pixel_array)\n",
        "                pixel_array = pixel_array.astype(np.uint8)\n",
        "\n",
        "            if pixel_array.ndim == 2: # Grayscale\n",
        "                pil_image = Image.fromarray(pixel_array).convert('RGB')\n",
        "            elif pixel_array.ndim == 3 and pixel_array.shape[-1] == 1: # Grayscale with channel dim\n",
        "                pil_image = Image.fromarray(pixel_array.squeeze(-1)).convert('RGB')\n",
        "            elif pixel_array.ndim == 3 and pixel_array.shape[-1] == 3: # RGB\n",
        "                pil_image = Image.fromarray(pixel_array)\n",
        "            elif pixel_array.ndim == 3 and pixel_array.shape[-1] == 4: # RGBA\n",
        "                pil_image = Image.fromarray(pixel_array).convert('RGB')\n",
        "            else:\n",
        "                print_once_dataset(f\"Warning: Unsupported DICOM pixel array shape {pixel_array.shape} for {dicom_path}. Trying to convert.\")\n",
        "                # Attempt to make it a 2D grayscale image if possible\n",
        "                if pixel_array.ndim > 2 : pixel_array = pixel_array[...,0] # take first channel or slice\n",
        "                if pixel_array.ndim > 2 : pixel_array = pixel_array[0] # take first frame\n",
        "                pil_image = Image.fromarray(pixel_array.astype(np.uint8)).convert('RGB')\n",
        "\n",
        "\n",
        "            # Use MedGemma's image_processor\n",
        "            # It expects a PIL Image or list of PIL Images.\n",
        "            # It handles resizing, normalization, and tensor conversion according to MedGemma's needs.\n",
        "            processed_output = self.processor.image_processor(images=pil_image, return_tensors=\"pt\")\n",
        "            pixel_values = processed_output.pixel_values.squeeze(0) # Remove batch dim\n",
        "            return pixel_values\n",
        "\n",
        "        except Exception as e:\n",
        "            print_once_dataset(f\"Error processing DICOM {dicom_path} with image_processor: {e}. Applying basic fallback.\")\n",
        "            # Fallback: create a dummy black image if processing fails\n",
        "            try:\n",
        "                # Try to load with PIL directly for basic transform\n",
        "                pil_image_fallback = Image.open(dicom_path).convert(\"RGB\") # This might fail for some DICOMs\n",
        "                return self.basic_transforms(pil_image_fallback)\n",
        "            except Exception as e_fallback:\n",
        "                print_once_dataset(f\"Fallback PIL loading also failed for {dicom_path}: {e_fallback}. Returning zero tensor.\")\n",
        "                return torch.zeros((3, self.target_size_ref[0], self.target_size_ref[1]))\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        image_path = row['image_path']\n",
        "        ldl_scaled = row['LDL_scaled'] # Target variable\n",
        "\n",
        "        pixel_values = self.load_and_preprocess_dicom(image_path)\n",
        "        target_ldl_scaled = torch.tensor(ldl_scaled, dtype=torch.float32)\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"labels\": target_ldl_scaled.unsqueeze(0) # Ensure target is (1,) for MSELoss\n",
        "        }\n",
        "\n",
        "# --- Create Datasets ---\n",
        "# `tokenizer` from Cell 0.3 is MedGemma's processor\n",
        "# `TARGET_SIZE_FOR_IMAGES` from Cell 5 is a reference\n",
        "if 'train_df' in locals() and not train_df.empty and 'tokenizer' in locals() and tokenizer is not None:\n",
        "    train_dataset = MedGemmaVisionDataset(train_df, tokenizer, TARGET_SIZE_FOR_IMAGES)\n",
        "    print(f\"Train dataset created with {len(train_dataset)} samples.\")\n",
        "else:\n",
        "    train_dataset = None\n",
        "    print(\"Could not create train_dataset. Check train_df and tokenizer.\")\n",
        "\n",
        "if 'val_df' in locals() and not val_df.empty and 'tokenizer' in locals() and tokenizer is not None:\n",
        "    val_dataset = MedGemmaVisionDataset(val_df, tokenizer, TARGET_SIZE_FOR_IMAGES)\n",
        "    print(f\"Validation dataset created with {len(val_dataset)} samples.\")\n",
        "else:\n",
        "    val_dataset = None\n",
        "    print(\"Could not create val_dataset. Check val_df and tokenizer.\")\n",
        "\n",
        "# Example: Fetch one item to test\n",
        "if train_dataset:\n",
        "    print(\"\\nSample from train_dataset:\")\n",
        "    try:\n",
        "        sample = train_dataset[0]\n",
        "        for key, val in sample.items():\n",
        "            print(f\"  {key}: shape {val.shape}, dtype {val.dtype}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching sample from train_dataset: {e}\")\n",
        "        print(\"This might indicate issues with DICOM loading or processing in your dataset.\")\n",
        "\n",
        "print(\"\\nCell 7: MedGemmaVisionDataset class defined and datasets instantiated.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard PyTorch collate_fn should work if items are already tensors.\n",
        "def vision_collate_fn(batch):\n",
        "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
        "    labels = torch.stack([item['labels'] for item in batch])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n",
        "BATCH_SIZE = 8 # Adjust based on GPU memory (e.g., 4, 8, 16)\n",
        "\n",
        "if train_dataset:\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        collate_fn=vision_collate_fn, # Use custom collate\n",
        "        num_workers=2, # Use multiple workers for faster data loading if not on Windows/debugging\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "    print(f\"\\nTrain DataLoader created. Batches per epoch: {len(train_loader)}\")\n",
        "else:\n",
        "    train_loader = None\n",
        "\n",
        "if val_dataset:\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False, # No need to shuffle validation data\n",
        "        collate_fn=vision_collate_fn,\n",
        "        num_workers=2,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "    print(f\"Validation DataLoader created. Batches per epoch: {len(val_loader)}\")\n",
        "else:\n",
        "    val_loader = None\n",
        "\n",
        "# Test one batch from train_loader\n",
        "if train_loader:\n",
        "    print(\"\\nSample batch from train_loader:\")\n",
        "    try:\n",
        "        batch_sample = next(iter(train_loader))\n",
        "        for key, val in batch_sample.items():\n",
        "            print(f\"  {key}: shape {val.shape}, dtype {val.dtype}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching batch from train_loader: {e}\")\n",
        "\n",
        "print(\"\\nCell 8: DataLoaders created.\")"
      ],
      "metadata": {
        "id": "cSq1JoFRvFZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Training Setup (Optimizer, Loss, Learning Rate) - MODIFIED\n",
        "\n",
        "\"\"\"\n",
        "import torch.optim as optim\n",
        "\n",
        "LEARNING_RATE = 5e-5 # Common starting point for LoRA fine-tuning. May need adjustment.\n",
        "EPOCHS = 10 # Start with a moderate number, e.g., 5-20.\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "if regressor_model is not None: # Ensure the model was created in Cell 0.5\n",
        "    optimizer = optim.AdamW(regressor_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # --- STRATEGY 1: Attempt to cast the entire regressor_model to bfloat16 ---\n",
        "    # This assumes `regressor_model` is already on the correct `device` (e.g., 'cuda')\n",
        "    # And `model_dtype` (e.g. torch.bfloat16) should be what Unsloth set for the base model.\n",
        "\n",
        "    # Get the dtype from the Unsloth-loaded base model component within regressor_model\n",
        "    # This is the most reliable source for the target dtype.\n",
        "    if hasattr(regressor_model, 'medgemma_model') and hasattr(regressor_model.medgemma_model, 'dtype'):\n",
        "        target_model_dtype = regressor_model.medgemma_model.dtype\n",
        "        print(f\"\\nTarget dtype for model components (from base Unsloth model): {target_model_dtype}\")\n",
        "\n",
        "        if target_model_dtype == torch.bfloat16:\n",
        "            print(f\"Attempting to cast entire regressor_model and its submodules to {target_model_dtype} (Strategy 1)...\")\n",
        "            try:\n",
        "                # This will attempt to cast all parameters and buffers.\n",
        "                regressor_model = regressor_model.to(dtype=target_model_dtype)\n",
        "                print(\"Casting of entire regressor_model to bfloat16 attempted.\")\n",
        "\n",
        "                # Optional: Verification - Check dtypes of some parameters\n",
        "                # print(\"Verifying some parameter dtypes after full model cast:\")\n",
        "                # for name, param in regressor_model.named_parameters():\n",
        "                #     if \"lora\" in name.lower() or \"regression_head\" in name.lower() or \"bias\" in name.lower(): # Check some key ones\n",
        "                #         if param.numel() > 0: # Only print if param is not empty\n",
        "                #             print(f\"  Param: {name[:60]}..., Dtype: {param.dtype}, Device: {param.device}\")\n",
        "                #         break # Just check a few to avoid too much output\n",
        "            except Exception as e_cast_full:\n",
        "                print(f\"ERROR during full regressor_model.to(dtype={target_model_dtype}): {e_cast_full}\")\n",
        "                print(\"Full model cast failed. Proceeding without it, relying on input tensor casting in training loop.\")\n",
        "        else:\n",
        "            print(f\"Base model dtype is {target_model_dtype}, not bfloat16. Skipping full model bfloat16 cast strategy.\")\n",
        "    else:\n",
        "        print(\"\\nCould not reliably determine target_model_dtype from regressor_model.medgemma_model.dtype.\")\n",
        "        print(\"Skipping full model cast strategy. Will rely on input tensor casting in training loop.\")\n",
        "    # --- END OF STRATEGY 1 ---\n",
        "\n",
        "    print(f\"\\nOptimizer: AdamW, LR: {LEARNING_RATE}, Weight Decay: {WEIGHT_DECAY}\")\n",
        "    print(f\"Loss Function: MSELoss\")\n",
        "    print(f\"Training for {EPOCHS} epochs.\")\n",
        "else:\n",
        "    print(\"CRITICAL ERROR: regressor_model is None (was not created in Cell 0.5). Cannot set up optimizer and loss.\")\n",
        "    optimizer = None\n",
        "    criterion = None\n",
        "\"\"\"\n",
        "\n",
        "# Cell 9: Training Setup (Optimizer, Loss, Learning Rate) - REVISED FOR FLOAT16\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "LEARNING_RATE = 5e-5\n",
        "EPOCHS = 10\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "if regressor_model is not None:\n",
        "    optimizer = optim.AdamW(regressor_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    if hasattr(regressor_model, 'medgemma_model') and hasattr(regressor_model.medgemma_model, 'dtype'):\n",
        "        target_dtype_for_components = regressor_model.medgemma_model.dtype # Should be float16\n",
        "        print(f\"\\nTarget dtype for all model components (from base Unsloth model): {target_dtype_for_components}\")\n",
        "\n",
        "        if target_dtype_for_components == torch.float16: # Check for float16\n",
        "            print(f\"Attempting to ensure all components of regressor_model are on {target_dtype_for_components}...\")\n",
        "            try:\n",
        "                regressor_model = regressor_model.to(dtype=target_dtype_for_components)\n",
        "                print(f\"Casting of entire regressor_model to {target_dtype_for_components} completed.\")\n",
        "                # Verification\n",
        "                # print(\"Verifying select parameter dtypes after full model cast:\")\n",
        "                # for name, param in regressor_model.named_parameters():\n",
        "                #     if param.requires_grad and (\"lora\" in name.lower() or \"regression_head\" in name.lower()):\n",
        "                #         if param.numel() > 0:\n",
        "                #              print(f\"  Trainable Param: {name[:70]}..., Dtype: {param.dtype}, Device: {param.device}\")\n",
        "            except Exception as e_cast_all:\n",
        "                print(f\"ERROR during full regressor_model.to(dtype={target_dtype_for_components}): {e_cast_all}\")\n",
        "        else:\n",
        "            print(f\"Base model dtype is {target_dtype_for_components}, not float16. Current strategy might need adjustment.\")\n",
        "    else:\n",
        "        print(\"\\nCould not reliably determine target_dtype for component casting.\")\n",
        "\n",
        "    print(f\"\\nOptimizer: AdamW, LR: {LEARNING_RATE}, Weight Decay: {WEIGHT_DECAY}\")\n",
        "    print(f\"Loss Function: MSELoss\")\n",
        "    print(f\"Training for {EPOCHS} epochs.\")\n",
        "else:\n",
        "    print(\"CRITICAL ERROR: regressor_model is None. Cannot set up optimizer and loss.\")\n",
        "    optimizer = None; criterion = None"
      ],
      "metadata": {
        "id": "xYo9BAroWReB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Training and Evaluation Loop - MODIFIED\n",
        "\"\"\"\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "PATIENCE_EPOCHS = 3 # For early stopping if validation loss doesn't improve\n",
        "\n",
        "# Helper for printing messages only once during training loop\n",
        "printed_messages_train_loop = set() # Use a different name to avoid conflict if re-running cells\n",
        "def print_once_train_loop(message):\n",
        "    global printed_messages_train_loop\n",
        "    if message not in printed_messages_train_loop:\n",
        "        print(message)\n",
        "        printed_messages_train_loop.add(message)\n",
        "\n",
        "if regressor_model is not None and train_loader is not None and val_loader is not None and optimizer is not None and criterion is not None:\n",
        "    print(f\"\\nStarting training on device: {device}...\") # device was set in Cell 0.5\n",
        "\n",
        "    # Determine the model's expected input dtype (should be bfloat16 if Unsloth set it)\n",
        "    # This comes from the base Unsloth model component\n",
        "    if hasattr(regressor_model, 'medgemma_model') and hasattr(regressor_model.medgemma_model, 'dtype'):\n",
        "        model_input_dtype = regressor_model.medgemma_model.dtype\n",
        "    else:\n",
        "        # Fallback if attribute not found, assume bfloat16 based on previous errors\n",
        "        print_once_train_loop(\"Warning: Could not directly get model_input_dtype from regressor_model.medgemma_model.dtype. Assuming torch.bfloat16.\")\n",
        "        model_input_dtype = torch.bfloat16\n",
        "\n",
        "    print(f\"Model's expected input dtype for pixel_values: {model_input_dtype}\")\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        regressor_model.train()\n",
        "        running_train_loss = 0.0\n",
        "        processed_batches_train = 0\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            # Data from DataLoader is typically float32\n",
        "            pixel_values_f32 = batch['pixel_values'].to(device)\n",
        "            labels_f32 = batch['labels'].to(device) # Labels for MSELoss are typically Float32\n",
        "\n",
        "            # Explicitly cast pixel_values to the model's expected input dtype\n",
        "            pixel_values_casted = pixel_values_f32.to(model_input_dtype)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            try:\n",
        "                # Forward pass with casted input\n",
        "                predictions = regressor_model(pixel_values_casted)\n",
        "\n",
        "                # Predictions will likely be in model_input_dtype (e.g., bfloat16).\n",
        "                # MSELoss can often handle mixed precision (e.g., bfloat16 pred, float32 label).\n",
        "                # If criterion errors on dtype, cast predictions: loss = criterion(predictions.to(torch.float32), labels_f32)\n",
        "                loss = criterion(predictions, labels_f32)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_train_loss += loss.item()\n",
        "                processed_batches_train += 1\n",
        "\n",
        "                if (i + 1) % 20 == 0 or (i + 1) == len(train_loader):\n",
        "                    print(f\"Epoch [{epoch+1}/{EPOCHS}], Batch [{i+1}/{len(train_loader)}], Train Loss: {loss.item():.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print_once_train_loop(f\"ERROR during training forward/backward pass at batch {i}: {e}\")\n",
        "                if \"expected scalar type\" in str(e).lower():\n",
        "                    print_once_train_loop(f\"  Input pixel_values_casted dtype: {pixel_values_casted.dtype}\")\n",
        "                    # If predictions object exists before error:\n",
        "                    if 'predictions' in locals() and isinstance(predictions, torch.Tensor):\n",
        "                         print_once_train_loop(f\"  Predictions (if formed) dtype: {predictions.dtype}\")\n",
        "                # To get more details on where exactly the error occurs inside the model:\n",
        "                # import traceback\n",
        "                # print_once_train_loop(traceback.format_exc())\n",
        "                continue # Skip this batch and try the next one\n",
        "\n",
        "        epoch_train_loss = running_train_loss / processed_batches_train if processed_batches_train > 0 else 0.0\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        print(f\"Epoch [{epoch+1}/{EPOCHS}] - Average Training Loss: {epoch_train_loss:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        regressor_model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        processed_batches_val = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_val in val_loader:\n",
        "                pixel_values_f32_val = batch_val['pixel_values'].to(device)\n",
        "                labels_f32_val = batch_val['labels'].to(device)\n",
        "\n",
        "                pixel_values_casted_val = pixel_values_f32_val.to(model_input_dtype)\n",
        "\n",
        "                try:\n",
        "                    predictions_val = regressor_model(pixel_values_casted_val)\n",
        "                    loss_val = criterion(predictions_val, labels_f32_val)\n",
        "                    running_val_loss += loss_val.item()\n",
        "                    processed_batches_val +=1\n",
        "                except Exception as e_val:\n",
        "                    print_once_train_loop(f\"ERROR during validation forward pass: {e_val}\")\n",
        "                    continue\n",
        "\n",
        "        epoch_val_loss = running_val_loss / processed_batches_val if processed_batches_val > 0 else 0.0\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        print(f\"Epoch [{epoch+1}/{EPOCHS}] - Average Validation Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            best_val_loss = epoch_val_loss\n",
        "            patience_counter = 0\n",
        "            save_dir = \"./best_model_checkpoint\"\n",
        "            if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
        "\n",
        "            # Save LoRA adapters from the PEFT-adapted model component\n",
        "            if hasattr(regressor_model, 'medgemma_model') and hasattr(regressor_model.medgemma_model, 'save_pretrained'):\n",
        "                regressor_model.medgemma_model.save_pretrained(os.path.join(save_dir, \"lora_adapters\"))\n",
        "                print(f\"Saved LoRA adapters at epoch {epoch+1}.\")\n",
        "            else:\n",
        "                print_once_train_loop(\"Could not save LoRA adapters: regressor_model.medgemma_model.save_pretrained not found.\")\n",
        "\n",
        "            # Save the state of the regression head\n",
        "            if hasattr(regressor_model, 'regression_head'):\n",
        "                torch.save(regressor_model.regression_head.state_dict(), os.path.join(save_dir, \"regression_head.pth\"))\n",
        "                print(f\"Saved regression head state at epoch {epoch+1}.\")\n",
        "            else:\n",
        "                print_once_train_loop(\"Could not save regression head: regressor_model.regression_head not found.\")\n",
        "            print(f\"Validation loss improved to {best_val_loss:.4f}. Saved best model components.\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        print(\"-\" * 30)\n",
        "        if patience_counter >= PATIENCE_EPOCHS:\n",
        "            print(f\"Early stopping triggered after {PATIENCE_EPOCHS} epochs without improvement on validation loss.\")\n",
        "            break\n",
        "    print(\"Training complete.\")\n",
        "else:\n",
        "    print(\"Cannot start training. One or more critical components (model, dataloaders, optimizer, criterion) are missing.\")\n",
        "\n",
        "# Plotting training and validation loss\n",
        "if train_losses and val_losses:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss (MSE)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\"\"\"\n",
        "import torch.amp\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "PATIENCE_EPOCHS = 3\n",
        "\n",
        "printed_messages_train_loop = set()\n",
        "def print_once_train_loop(message):\n",
        "    global printed_messages_train_loop\n",
        "    if message not in printed_messages_train_loop:\n",
        "        print(message)\n",
        "        printed_messages_train_loop.add(message)\n",
        "\n",
        "if regressor_model is not None and train_loader is not None and val_loader is not None and optimizer is not None and criterion is not None:\n",
        "    print(f\"\\nStarting training on device: {device}...\")\n",
        "\n",
        "    # Model activation dtype should now be float16\n",
        "    if hasattr(regressor_model, 'medgemma_model') and hasattr(regressor_model.medgemma_model, 'dtype'):\n",
        "        model_activation_dtype = regressor_model.medgemma_model.dtype # Should be torch.float16\n",
        "    else:\n",
        "        print_once_train_loop(\"Warning: Could not get model_activation_dtype. Assuming torch.float16.\")\n",
        "        model_activation_dtype = torch.float16\n",
        "\n",
        "    print(f\"Model's activation dtype (for input pixel_values): {model_activation_dtype}\")\n",
        "\n",
        "    # Autocast will now use float16 on CUDA for T4\n",
        "    autocast_dtype = torch.float16 # Explicitly use float16 for T4\n",
        "    print(f\"Using torch.amp.autocast with dtype: {autocast_dtype} on device type: {device.type}\")\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        regressor_model.train()\n",
        "        running_train_loss = 0.0\n",
        "        processed_batches_train = 0\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            pixel_values_f32 = batch['pixel_values'].to(device)\n",
        "            labels_f32 = batch['labels'].to(device)\n",
        "\n",
        "            # Cast input to float16\n",
        "            pixel_values_casted_for_input = pixel_values_f32.to(model_activation_dtype) # model_activation_dtype is float16\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.amp.autocast(device_type=device.type, dtype=autocast_dtype, enabled=True):\n",
        "                try:\n",
        "                    predictions = regressor_model(pixel_values_casted_for_input)\n",
        "                    loss = criterion(predictions.to(torch.float32), labels_f32) # Keep predictions to float32 for robust loss\n",
        "                except Exception as e:\n",
        "                    print_once_train_loop(f\"ERROR during training forward pass (inside autocast) at batch {i}: {e}\")\n",
        "                    # import traceback # Uncomment for full traceback\n",
        "                    # print_once_train_loop(traceback.format_exc())\n",
        "                    continue\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_train_loss += loss.item()\n",
        "            processed_batches_train += 1\n",
        "\n",
        "            if (i + 1) % 20 == 0 or (i + 1) == len(train_loader):\n",
        "                print(f\"Epoch [{epoch+1}/{EPOCHS}], Batch [{i+1}/{len(train_loader)}], Train Loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_train_loss = running_train_loss / processed_batches_train if processed_batches_train > 0 else 0.0\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        print(f\"Epoch [{epoch+1}/{EPOCHS}] - Average Training Loss: {epoch_train_loss:.4f}\")\n",
        "\n",
        "        regressor_model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        processed_batches_val = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_val in val_loader:\n",
        "                pixel_values_f32_val = batch_val['pixel_values'].to(device)\n",
        "                labels_f32_val = batch_val['labels'].to(device)\n",
        "                pixel_values_casted_for_input_val = pixel_values_f32_val.to(model_activation_dtype)\n",
        "\n",
        "                with torch.amp.autocast(device_type=device.type, dtype=autocast_dtype, enabled=True):\n",
        "                    try:\n",
        "                        predictions_val = regressor_model(pixel_values_casted_for_input_val)\n",
        "                        loss_val = criterion(predictions_val.to(torch.float32), labels_f32_val)\n",
        "                    except Exception as e_val:\n",
        "                        print_once_train_loop(f\"ERROR during validation forward pass (inside autocast): {e_val}\")\n",
        "                        continue\n",
        "                running_val_loss += loss_val.item()\n",
        "                processed_batches_val +=1\n",
        "\n",
        "        epoch_val_loss = running_val_loss / processed_batches_val if processed_batches_val > 0 else 0.0\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        print(f\"Epoch [{epoch+1}/{EPOCHS}] - Average Validation Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            best_val_loss = epoch_val_loss; patience_counter = 0\n",
        "            save_dir = \"./best_model_checkpoint\"\n",
        "            if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
        "            if hasattr(regressor_model, 'medgemma_model') and hasattr(regressor_model.medgemma_model, 'save_pretrained'):\n",
        "                regressor_model.medgemma_model.save_pretrained(os.path.join(save_dir, \"lora_adapters\"))\n",
        "            if hasattr(regressor_model, 'regression_head'):\n",
        "                torch.save(regressor_model.regression_head.state_dict(), os.path.join(save_dir, \"regression_head.pth\"))\n",
        "            print(f\"Validation loss improved to {best_val_loss:.4f}. Saved best model components at epoch {epoch+1}.\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        print(\"-\" * 30)\n",
        "        if patience_counter >= PATIENCE_EPOCHS:\n",
        "            print(f\"Early stopping triggered.\")\n",
        "            break\n",
        "    print(\"Training complete.\")\n",
        "else:\n",
        "    print(\"Cannot start training. Critical components missing.\")\n",
        "\n",
        "if train_losses and val_losses: # Plotting\n",
        "    plt.figure(figsize=(10,5)); plt.plot(train_losses, label='Training Loss'); plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Loss Over Epochs'); plt.xlabel('Epochs'); plt.ylabel('Loss (MSE)'); plt.legend(); plt.grid(True); plt.show()"
      ],
      "metadata": {
        "id": "IvYqs994WWHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The best model was saved during training. Here's how you might save the *final* model\n",
        "# if you didn't use early stopping or want the model from the last epoch.\n",
        "\n",
        "final_model_save_path = \"./final_model_checkpoint\"\n",
        "if regressor_model is not None and os.path.exists(\"./best_model_checkpoint\"): # Check if best model was saved\n",
        "    print(f\"\\nBest model was saved during training to ./best_model_checkpoint\")\n",
        "    print(\"To use the best model, load from './best_model_checkpoint/lora_adapters' and './best_model_checkpoint/regression_head.pth'\")\n",
        "elif regressor_model is not None: # Save final model if no best model path exists (e.g. early stopping not triggered or not implemented fully)\n",
        "    if not os.path.exists(final_model_save_path): os.makedirs(final_model_save_path)\n",
        "    print(f\"\\nSaving final model to {final_model_save_path}...\")\n",
        "    # Save LoRA adapters of the base MedGemma model\n",
        "    regressor_model.medgemma_model.save_pretrained(os.path.join(final_model_save_path, \"lora_adapters\"))\n",
        "    # Save the state of the regression head\n",
        "    torch.save(regressor_model.regression_head.state_dict(), os.path.join(final_model_save_path, \"regression_head.pth\"))\n",
        "    print(f\"Final LoRA adapters saved to {os.path.join(final_model_save_path, 'lora_adapters')}\")\n",
        "    print(f\"Final regression head state saved to {os.path.join(final_model_save_path, 'regression_head.pth')}\")\n",
        "else:\n",
        "    print(\"\\nNo model to save or best model already indicated.\")\n",
        "\n",
        "\n",
        "# --- How to load the saved (best or final) model for inference ---\n",
        "# This demonstrates loading the components back.\n",
        "\n",
        "# 1. Define the path to your saved components (e.g., best model)\n",
        "saved_lora_path = \"./best_model_checkpoint/lora_adapters\" # Or final_model_save_path + \"/lora_adapters\"\n",
        "saved_head_path = \"./best_model_checkpoint/regression_head.pth\" # Or final_model_save_path + \"/regression_head.pth\"\n",
        "\n",
        "if os.path.exists(saved_lora_path) and os.path.exists(saved_head_path) and vision_feature_dim is not None:\n",
        "    print(f\"\\n--- Example: Loading saved model components from {saved_lora_path} and {saved_head_path} ---\")\n",
        "    # A. Load the base MedGemma model (without PEFT initially, or it will try to load adapters from original HF name)\n",
        "    #    It's often cleaner to load the base and then apply PEFT adapters.\n",
        "    #    However, Unsloth's `from_pretrained` on a PEFT saved path should work.\n",
        "\n",
        "    print(f\"Loading base MedGemma model ({selected_model_name}) and then applying saved LoRA adapters from {saved_lora_path}...\")\n",
        "\n",
        "    loaded_base_model, loaded_tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=selected_model_name, # Start with the original base model name\n",
        "        max_seq_length=2048,\n",
        "        dtype=None,\n",
        "        load_in_4bit=True,\n",
        "        # token = \"hf_...\"\n",
        "    )\n",
        "\n",
        "    # Now, apply the saved LoRA adapters\n",
        "    # Important: The `PeftModel.from_pretrained` expects the *base model* and the path to adapters.\n",
        "    from peft import PeftModel\n",
        "    loaded_peft_medgemma_model = PeftModel.from_pretrained(loaded_base_model, saved_lora_path)\n",
        "    print(\"PEFT MedGemma model with saved LoRA adapters loaded.\")\n",
        "\n",
        "    # B. Instantiate your RegressorModel wrapper with the loaded PEFT MedGemma\n",
        "    loaded_regressor_model = MedGemmaVisionRegressor(loaded_peft_medgemma_model, vision_feature_dim)\n",
        "\n",
        "    # C. Load the state_dict for the regression head\n",
        "    loaded_regressor_model.regression_head.load_state_dict(torch.load(saved_head_path, map_location=device))\n",
        "    print(\"Regression head state loaded.\")\n",
        "\n",
        "    loaded_regressor_model.to(device)\n",
        "    loaded_regressor_model.eval() # Set to evaluation mode\n",
        "    print(\"Complete RegressorModel loaded and ready for inference.\")\n",
        "\n",
        "    # Example inference (requires a sample from val_loader or test_loader)\n",
        "    if val_loader:\n",
        "        try:\n",
        "            sample_batch_inference = next(iter(val_loader))\n",
        "            pixel_values_inf = sample_batch_inference['pixel_values'].to(device)\n",
        "            labels_inf = sample_batch_inference['labels'].to(device)\n",
        "            with torch.no_grad():\n",
        "                predictions_inf = loaded_regressor_model(pixel_values_inf)\n",
        "            print(f\"\\nSample inference output shape: {predictions_inf.shape}\")\n",
        "            # You would then unscale predictions using ldl_scaler.inverse_transform()\n",
        "            if ldl_scaler:\n",
        "                 predicted_ldl_original_scale = ldl_scaler.inverse_transform(predictions_inf.cpu().numpy())\n",
        "                 actual_ldl_original_scale = ldl_scaler.inverse_transform(labels_inf.cpu().numpy())\n",
        "                 print(f\"Sample predictions (original scale): {predicted_ldl_original_scale[:5].flatten()}\")\n",
        "                 print(f\"Sample actuals (original scale):    {actual_ldl_original_scale[:5].flatten()}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during sample inference with loaded model: {e}\")\n",
        "else:\n",
        "    print(\"\\nSkipping demonstration of loading model as saved paths or vision_feature_dim not found.\")\n",
        "\n",
        "\n",
        "print(\"\\nCell 11: Model saving and loading example complete.\")\n",
        "print(\"\\n--- End of Script ---\")"
      ],
      "metadata": {
        "id": "VbHPLQdsWa2c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}