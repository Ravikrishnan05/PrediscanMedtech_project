{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMPytYS0MAy4NcEJE+hl/Ku",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravikrishnan05/PrediscanMedtech_project/blob/main/Unsloth_ptmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4A4G9LfFoEEg"
      },
      "outputs": [],
      "source": [
        "# To run this, press \"Runtime\" and press \"Run all\" on a free Tesla T4 Google Colab instance!\n",
        "\n",
        "#    Join Discord if you need help + ⭐ Star us on Github ⭐\n",
        "# To install Unsloth on your own computer, follow the installation instructions on our Github page here.\n",
        "\n",
        "# You will learn how to do data prep, how to train, how to run the model, & how to save it\n",
        "\n",
        "# News\n",
        "# Unsloth now supports Text-to-Speech (TTS) models. Read our guide here.\n",
        "\n",
        "# Read our Qwen3 Guide and check out our new Dynamic 2.0 quants which outperforms other quantization methods!\n",
        "\n",
        "# Visit our docs for all our model uploads and notebooks.\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Cell 0.1: Unsloth Installation\n",
        "# -----------------------------------------------------------------------------\n",
        "#%%capture # Use %%capture to hide pip outputs if desired\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    print(\"Installing Unsloth for local environment...\")\n",
        "    !pip install \"unsloth[colab-new]@git+https://github.com/unslothai/unsloth.git\" # Adapting for general install\n",
        "else:\n",
        "    print(\"Installing Unsloth for Colab environment...\")\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install \"unsloth[colab-new]@git+https://github.com/unslothai/unsloth.git\"\n",
        "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "    !pip install --no-deps \"unsloth[colab-new]@git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Cell 0.2: Additional Library Installations\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\nInstalling additional libraries for data processing and DICOM handling...\")\n",
        "!pip install -q pydicom pandas opencv-python Pillow scikit-learn matplotlib seaborn \"huggingface_hub>=0.23.0\" \"hf_transfer>=0.1.6\" \"datasets>=2.16.0\" sentencepiece protobuf"
      ],
      "metadata": {
        "id": "fhJQhFboqc1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e6100bd"
      },
      "source": [
        "# Install unsloth_zoo\n",
        "print(\"\\nInstalling unsloth_zoo...\")\n",
        "!pip install unsloth_zoo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unsloth FastModel supports loading nearly any model now! This includes Vision and Text models!\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Cell 0.3: Unsloth Model Loading\n",
        "# -----------------------------------------------------------------------------\n",
        "from unsloth import FastLanguageModel # Changed from FastModel to FastLanguageModel as per recent Unsloth examples for language models\n",
        "import torch"
      ],
      "metadata": {
        "id": "aTMppuNfrAxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: MODEL SELECTION FOR YOUR TASK\n",
        "# The model \"unsloth/gemma-3-4b-it\" is a TEXT-BASED instruct model.\n",
        "# Your original code used MedGemma, a VISION-LANGUAGE model, and processed images.\n",
        "# If your task involves processing images to predict LDL, you MUST select a vision-language model.\n",
        "# Examples:\n",
        "#   - Search for Unsloth-quantized vision models: https://huggingface.co/unsloth\n",
        "#   - Try loading a standard HF vision model (e.g., \"google/medgemma-4b-pt\", \"llava-hf/llava-1.5-7b-hf\", \"microsoft/phi-3-vision-128k-instruct\")\n",
        "#     FastLanguageModel might support them. If so, set `finetune_vision_layers = True` in the PEFT setup.\n",
        "# For this example, we'll use the text model from the Unsloth template.\n",
        "# You will need to adapt your data processing (especially image handling in the Dataset)\n",
        "# if you use a text model for a vision task, or change the model_name.\n",
        "\n",
        "# List of potential models (mostly text, check for vision capabilities if needed)\n",
        "fourbit_models = [\n",
        "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\", # Current choice\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
        "    \"unsloth/Phi-4\",\n",
        "    # Potentially add your vision model here if Unsloth supports it directly or via HF path\n",
        "    # \"google/medgemma-4b-pt\", # Example: if you want to try loading your original model\n",
        "]\n",
        "\n",
        "#selected_model_name = \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\" # Defaulting to Unsloth's example text model\n",
        "selected_model_name = \"google/medgemma-4b-pt\" # << TRY THIS FOR YOUR VISION TASK (or other vision model)\n",
        "\n",
        "print(f\"Attempting to load model: {selected_model_name}\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = selected_model_name,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None, # None for auto detection. torch.float16 if Ampere+. torch.bfloat16 if Hopper+\n",
        "    load_in_4bit = True,\n",
        "    # token = \"hf_...\", # use one if using gated models like Llama\n",
        ")\n",
        "print(f\"Model {selected_model_name} loaded.\")\n",
        "\n",
        "# Note: For vision models, the 'tokenizer' might be a composite object\n",
        "# or you might access an image processor via `model.processor` or `tokenizer.image_processor`.\n",
        "# This depends on how Unsloth handles vision models."
      ],
      "metadata": {
        "id": "5AmBkBZurmmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Cell 0.4: Unsloth PEFT (LoRA) Setup\n",
        "# -----------------------------------------------------------------------------\n",
        "# We now add LoRA adapters so we only need to update a small amount of parameters!\n",
        "model = FastLanguageModel.get_peft_model( # Changed from add_lora_weights to get_peft_model\n",
        "    model,\n",
        "    finetune_vision_layers = True, # Set to True if you load a vision model and want to finetune vision components\n",
        "                                    # This parameter might not be directly in add_lora_weights,\n",
        "                                    # target_modules selection would handle this.\n",
        "    r = 8,\n",
        "    lora_alpha = 8, # Recommended alpha == r or 2*r\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 42,\n",
        "    # target_modules:\n",
        "    # For Gemma: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "    # For Llama: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "    # For Mistral: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "    # For Phi-3: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"dense\", \"fc1\", \"fc2\"]\n",
        "    # If using a vision model, you might need to identify and add its vision-specific modules to target_modules.\n",
        "    # Unsloth might auto-detect common ones for known architectures.\n",
        "    # Example for Llava (conceptual): target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"vision_tower.vision_model...\"]\n",
        "    target_modules = None, # None for Unsloth's automatic selection based on model type\n",
        "                           # If None, Unsloth will try to find all linear layers\n",
        ")\n",
        "print(\"LoRA adapters added to the model.\")"
      ],
      "metadata": {
        "id": "72tLTX8wr9J0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Cell 1: PyTorch/HuggingFace Imports and Setup (Adapted from user's Cell 1)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\nImporting libraries...\")\n",
        "# Python Standard Libraries\n",
        "import shutil # os, zipfile already imported or not needed here\n",
        "import zipfile\n",
        "\n",
        "# Third-party Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pydicom\n",
        "import cv2 # OpenCV\n",
        "from PIL import Image\n",
        "\n",
        "# PyTorch\n",
        "# import torch # Already imported\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Hugging Face (tokenizer is already loaded by Unsloth)\n",
        "# from transformers import AutoProcessor # Replaced by Unsloth's tokenizer\n",
        "\n",
        "# Plotting (optional, but often useful)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Colab specific\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"--- Library Version Checks ---\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "import sklearn\n",
        "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
        "# print(f\"TensorFlow Version: {tf.__version__}\") # TensorFlow not used in this Unsloth/PyTorch setup\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"PyTorch CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU available for PyTorch: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"GPU not available for PyTorch, using CPU.\")\n",
        "\n",
        "# For reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "print(\"\\nCell 1: Imports and basic setup complete.\")"
      ],
      "metadata": {
        "id": "BUr3OCOxsODW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# Cell 2: Configuration and Unzip Data (From user's Cell 2)\n",
        "# --------------------------------------------------\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# --- Configuration ---\n",
        "DRIVE_CSV_PATH = \"/content/drive/MyDrive/cp.csv\"\n",
        "DRIVE_ZIP_PATH = \"/content/drive/MyDrive/1000-20250517T062750Z-1-001.zip\" # Your image ZIP on Drive\n",
        "\n",
        "LOCAL_EXTRACT_PATH = \"/content/medgemma_extracted_images\"\n",
        "LOCAL_IMAGES_ROOT = os.path.join(LOCAL_EXTRACT_PATH, \"1000\") # Adjusted to match your structure\n",
        "LOCAL_CSV_PATH = \"/content/medgemma_cp.csv\"\n",
        "\n",
        "# --- Unzip Data (if not already done or if re-running) ---\n",
        "if os.path.exists(DRIVE_CSV_PATH):\n",
        "    shutil.copy(DRIVE_CSV_PATH, LOCAL_CSV_PATH)\n",
        "    print(f\"CSV copied to {LOCAL_CSV_PATH}\")\n",
        "else:\n",
        "    print(f\"ERROR: CSV file not found at {DRIVE_CSV_PATH}\")\n",
        "\n",
        "if os.path.exists(LOCAL_EXTRACT_PATH):\n",
        "    print(f\"Removing existing extraction directory: {LOCAL_EXTRACT_PATH}\")\n",
        "    shutil.rmtree(LOCAL_EXTRACT_PATH)\n",
        "os.makedirs(LOCAL_EXTRACT_PATH, exist_ok=True)\n",
        "print(f\"Created local extraction directory: {LOCAL_EXTRACT_PATH}\")\n",
        "\n",
        "if os.path.exists(DRIVE_ZIP_PATH):\n",
        "    print(f\"Unzipping {DRIVE_ZIP_PATH} to {LOCAL_EXTRACT_PATH}...\")\n",
        "    with zipfile.ZipFile(DRIVE_ZIP_PATH, 'r') as zip_ref:\n",
        "        zip_ref.extractall(LOCAL_EXTRACT_PATH)\n",
        "    print(\"Unzipping complete.\")\n",
        "    if os.path.exists(LOCAL_IMAGES_ROOT):\n",
        "        print(f\"Image root folder found at: {LOCAL_IMAGES_ROOT}\")\n",
        "    else:\n",
        "        print(f\"ERROR: Expected image root folder '{LOCAL_IMAGES_ROOT}' not found after unzipping. Check ZIP structure.\")\n",
        "        print(f\"Contents of {LOCAL_EXTRACT_PATH}: {os.listdir(LOCAL_EXTRACT_PATH)}\")\n",
        "\n",
        "else:\n",
        "    print(f\"ERROR: ZIP file not found at {DRIVE_ZIP_PATH}\")\n",
        "\n",
        "print(\"\\nCell 2: Data unzipping complete.\")\n"
      ],
      "metadata": {
        "id": "qKhbFStitDQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# Cell 3: Load and Filter Clinical Data to create image_df (From user's Cell 3)\n",
        "# --------------------------------------------------\n",
        "image_df = pd.DataFrame()\n",
        "\n",
        "if not os.path.exists(LOCAL_CSV_PATH):\n",
        "    print(f\"FATAL ERROR: Clinical CSV file not found at the expected local path: {LOCAL_CSV_PATH}\")\n",
        "else:\n",
        "    df_raw_from_cell3 = pd.read_csv(LOCAL_CSV_PATH)\n",
        "    print(f\"Initial number of rows in clinical data (Cell 3): {len(df_raw_from_cell3)}\")\n",
        "\n",
        "    person_id_col_name_c3 = 'person_id'\n",
        "    ldl_col_name_c3 = \"LDL Cholesterol Calculation (mg/dL)\" # Ensure this matches your CSV header\n",
        "\n",
        "    if not (person_id_col_name_c3 in df_raw_from_cell3.columns and ldl_col_name_c3 in df_raw_from_cell3.columns):\n",
        "        print(f\"ERROR: Required columns ('{person_id_col_name_c3}' or '{ldl_col_name_c3}') not found in CSV.\")\n",
        "        print(f\"Available columns: {df_raw_from_cell3.columns.tolist()}\")\n",
        "    else:\n",
        "        df_selected_c3 = df_raw_from_cell3[[person_id_col_name_c3, ldl_col_name_c3]].copy()\n",
        "        df_selected_c3.rename(columns={ldl_col_name_c3: 'LDL_temp'}, inplace=True)\n",
        "        df_selected_c3['LDL_temp'] = pd.to_numeric(df_selected_c3['LDL_temp'], errors='coerce')\n",
        "        df_selected_c3.dropna(subset=['LDL_temp'], inplace=True)\n",
        "        df_selected_c3 = df_selected_c3[df_selected_c3['LDL_temp'] > 0].copy()\n",
        "        df_selected_c3[person_id_col_name_c3] = df_selected_c3[person_id_col_name_c3].astype(str)\n",
        "        print(f\"Cleaned clinical data (positive LDLs only): {len(df_selected_c3)} records.\")\n",
        "\n",
        "        ldl_lookup_c3 = df_selected_c3.set_index(person_id_col_name_c3)['LDL_temp'].to_dict()\n",
        "\n",
        "        if not (os.path.exists(LOCAL_IMAGES_ROOT) and os.path.isdir(LOCAL_IMAGES_ROOT)):\n",
        "            print(f\"FATAL ERROR: Images root path '{LOCAL_IMAGES_ROOT}' does not exist or is not a directory.\")\n",
        "        else:\n",
        "            available_folders_c3 = set(os.listdir(LOCAL_IMAGES_ROOT))\n",
        "            valid_ids_clinical_c3 = set(ldl_lookup_c3.keys())\n",
        "            common_person_ids_c3 = sorted(list(valid_ids_clinical_c3 & available_folders_c3))\n",
        "            print(f\"Found {len(common_person_ids_c3)} common person_ids for mapping.\")\n",
        "\n",
        "            image_records_list = []\n",
        "            for pid_c3 in common_person_ids_c3:\n",
        "                folder_path_c3 = os.path.join(LOCAL_IMAGES_ROOT, pid_c3)\n",
        "                ldl_val_c3 = ldl_lookup_c3[pid_c3]\n",
        "                if os.path.isdir(folder_path_c3):\n",
        "                    for filename_c3 in os.listdir(folder_path_c3):\n",
        "                        if filename_c3.lower().endswith(\".dcm\"):\n",
        "                            image_path_c3 = os.path.join(folder_path_c3, filename_c3)\n",
        "                            image_records_list.append({\n",
        "                                \"person_id\": pid_c3,\n",
        "                                \"image_path\": image_path_c3,\n",
        "                                \"LDL\": ldl_val_c3\n",
        "                            })\n",
        "            image_df = pd.DataFrame(image_records_list)\n",
        "            if not image_df.empty:\n",
        "                print(f\"Final image_df created with {len(image_df)} image-LDL pairs.\")\n",
        "                from IPython.display import display # For better display in Colab\n",
        "                display(image_df.head())\n",
        "                print(f\"LDL stats in final image_df: min={image_df['LDL'].min()}, max={image_df['LDL'].max()}, mean={image_df['LDL'].mean()}\")\n",
        "            else:\n",
        "                print(\"WARNING: image_df is empty after mapping. Check paths, IDs, and DICOM file existence.\")\n",
        "print(\"\\nCell 3: image_df preparation complete.\")"
      ],
      "metadata": {
        "id": "nyRLqIrUtLLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Cell 4: Verify image_df (Adapted from user's Cell 4)\n",
        "# -----------------------------------------------------------------------------\n",
        "if 'image_df' in locals() and isinstance(image_df, pd.DataFrame) and not image_df.empty:\n",
        "    print(f\"\\nContinuing with 'image_df' which has {len(image_df)} records.\")\n",
        "    print(\"Columns in image_df:\", image_df.columns.tolist())\n",
        "    print(\"Sample of image_df:\")\n",
        "    from IPython.display import display\n",
        "    display(image_df.head())\n",
        "\n",
        "    required_cols = ['person_id', 'image_path', 'LDL']\n",
        "    if not all(col in image_df.columns for col in required_cols):\n",
        "        print(f\"ERROR: 'image_df' is missing one or more required columns: {required_cols}. Please re-run previous data preparation cells.\")\n",
        "    elif image_df['LDL'].min() <= 0: # type: ignore\n",
        "        print(f\"ERROR: 'image_df' still contains non-positive LDL values. LDL min: {image_df['LDL'].min()}. Please re-run filtering.\") # type: ignore\n",
        "    else:\n",
        "        print(\"'image_df' seems okay to proceed.\")\n",
        "else:\n",
        "    print(\"ERROR: 'image_df' not found or is empty. Please ensure your data preparation cells have been run successfully.\")\n",
        "    image_df = pd.DataFrame(columns=['person_id', 'image_path', 'LDL']) # Avoid NameError\n",
        "\n",
        "print(f\"\\nUsing Unsloth loaded model: {selected_model_name}\") # Was MEDGEMMA_PT_MODEL_ID\n",
        "print(\"\\nCell 4: image_df verification and Model ID check complete.\")"
      ],
      "metadata": {
        "id": "jKystT4Bt0Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Cell 5: Unsloth Tokenizer/Processor Info (Adapted from user's Cell 5)\n",
        "# -----------------------------------------------------------------------------\n",
        "# The `medgemma_processor` is now replaced by the `tokenizer` from Unsloth.\n",
        "# For vision models, this tokenizer might wrap an image processor,\n",
        "# or `model.processor` might be set by Unsloth.\n",
        "\n",
        "TARGET_SIZE_FOR_IMAGES = (896, 896) # Default, e.g. from MedGemma card\n",
        "print(f\"\\nDefault TARGET_SIZE_FOR_IMAGES: {TARGET_SIZE_FOR_IMAGES}\")\n",
        "\n",
        "if 'tokenizer' in locals() and tokenizer is not None:\n",
        "    print(f\"Unsloth tokenizer type: {type(tokenizer)}\")\n",
        "\n",
        "    image_processor_found = False\n",
        "    # Check 1: tokenizer.image_processor (common in Hugging Face for multimodal)\n",
        "    if hasattr(tokenizer, 'image_processor') and tokenizer.image_processor is not None:\n",
        "        print(\"Found `tokenizer.image_processor`.\")\n",
        "        try:\n",
        "            img_proc_size_info = tokenizer.image_processor.size\n",
        "            print(f\"  tokenizer.image_processor.size attribute: {img_proc_size_info}\")\n",
        "            # Parsing logic similar to your original cell\n",
        "            parsed_h, parsed_w = None, None\n",
        "            if isinstance(img_proc_size_info, dict): # e.g. {'height': H, 'width': W} or {'shortest_edge': S}\n",
        "                parsed_h = img_proc_size_info.get('height', img_proc_size_info.get('shortest_edge'))\n",
        "                if parsed_h is not None:\n",
        "                    parsed_w = img_proc_size_info.get('width', parsed_h if 'shortest_edge' in img_proc_size_info else None)\n",
        "            elif isinstance(img_proc_size_info, (list, tuple)) and len(img_proc_size_info) == 2: # e.g. (H, W)\n",
        "                parsed_h, parsed_w = img_proc_size_info[0], img_proc_size_info[1]\n",
        "            elif isinstance(img_proc_size_info, int): # e.g. S (square image)\n",
        "                parsed_h = parsed_w = img_proc_size_info\n",
        "\n",
        "            if parsed_h and parsed_w:\n",
        "                TARGET_SIZE_FOR_IMAGES = (parsed_h, parsed_w)\n",
        "                print(f\"  Target Image Size updated from tokenizer.image_processor: {TARGET_SIZE_FOR_IMAGES}\")\n",
        "                image_processor_found = True\n",
        "            else:\n",
        "                print(f\"  Could not reliably parse size from tokenizer.image_processor.size. Using default: {TARGET_SIZE_FOR_IMAGES}\")\n",
        "        except AttributeError:\n",
        "            print(\"  `tokenizer.image_processor` does not have a 'size' attribute or expected structure.\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Error accessing/parsing tokenizer.image_processor.size: {e}\")\n",
        "\n",
        "    # Check 2: model.processor (another place Unsloth might store it for vision models)\n",
        "    if not image_processor_found and hasattr(model, 'processor') and model.processor is not None:\n",
        "        # This assumes model.processor would be an image processor or a combined one\n",
        "        print(\"Found `model.processor`.\")\n",
        "        if hasattr(model.processor, 'image_processor') and model.processor.image_processor is not None: # If it's a combined processor\n",
        "            image_proc_component = model.processor.image_processor\n",
        "        else: # Or if model.processor *is* the image processor\n",
        "            image_proc_component = model.processor\n",
        "\n",
        "        if hasattr(image_proc_component, 'size'):\n",
        "            try:\n",
        "                img_proc_size_info = image_proc_component.size\n",
        "                print(f\"  model.processor's image component size attribute: {img_proc_size_info}\")\n",
        "                # (Add parsing logic here if different from above, otherwise reuse)\n",
        "                # For brevity, assuming similar parsing as above.\n",
        "                # If size is found and parsed, update TARGET_SIZE_FOR_IMAGES and set image_processor_found = True\n",
        "                # This part is illustrative; exact structure of model.processor varies.\n",
        "                print(f\"  (Parsing logic for model.processor.size would go here if needed)\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Error accessing/parsing model.processor's image component size: {e}\")\n",
        "        else:\n",
        "            print(\"  `model.processor` (or its image component) does not have a 'size' attribute.\")\n",
        "\n",
        "\n",
        "    if not image_processor_found:\n",
        "        print(f\"No image processor with size information explicitly found in Unsloth's tokenizer or model.processor.\")\n",
        "        print(f\"Using default TARGET_SIZE_FOR_IMAGES: {TARGET_SIZE_FOR_IMAGES}.\")\n",
        "        print(\"IMPORTANT: If you are using a vision model, ensure images are preprocessed to the model's expected input size.\")\n",
        "        print(f\"The current model '{selected_model_name}' is likely a TEXT model. Image processing capabilities might be limited or absent.\")\n",
        "\n",
        "else:\n",
        "    print(\"Unsloth tokenizer not loaded. Cannot determine image processing details.\")\n",
        "    print(f\"Using default TARGET_SIZE_FOR_IMAGES: {TARGET_SIZE_FOR_IMAGES}.\")\n",
        "\n",
        "print(\"\\nCell 5: Unsloth tokenizer/processor check complete.\")"
      ],
      "metadata": {
        "id": "KOsK3Cd4t4G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Cell 6: Data Splitting (Patient-Level) and LDL Normalization (From user's Cell 6)\n",
        "# -----------------------------------------------------------------------------\n",
        "train_df, val_df, test_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "ldl_scaler = None\n",
        "\n",
        "if 'image_df' in locals() and not image_df.empty:\n",
        "    print(f\"\\nStarting data splitting for {len(image_df)} image-LDL pairs...\")\n",
        "    if 'person_id' not in image_df.columns:\n",
        "        print(\"ERROR: 'person_id' column missing in image_df. Cannot perform patient-level split.\")\n",
        "    else:\n",
        "        unique_person_ids = image_df['person_id'].unique()\n",
        "        print(f\"Total unique patients for splitting: {len(unique_person_ids)}\")\n",
        "\n",
        "        if len(unique_person_ids) < 3:\n",
        "            print(\"Warning: Not enough unique patients for a robust 3-way (train/validation/test) split.\")\n",
        "            if len(unique_person_ids) == 2:\n",
        "                train_pids, val_pids = train_test_split(unique_person_ids, test_size=0.5, random_state=RANDOM_SEED)\n",
        "                test_pids = np.array([])\n",
        "            elif len(unique_person_ids) == 1:\n",
        "                train_pids = unique_person_ids\n",
        "                val_pids, test_pids = np.array([]), np.array([])\n",
        "            else:\n",
        "                train_pids, val_pids, test_pids = np.array([]), np.array([]), np.array([])\n",
        "        else:\n",
        "            train_pids, temp_pids = train_test_split(unique_person_ids, test_size=0.30, random_state=RANDOM_SEED)\n",
        "            if len(temp_pids) > 1 :\n",
        "                 val_pids, test_pids = train_test_split(temp_pids, test_size=0.50, random_state=RANDOM_SEED)\n",
        "            elif len(temp_pids) == 1:\n",
        "                val_pids = temp_pids\n",
        "                test_pids = np.array([])\n",
        "            else:\n",
        "                val_pids, test_pids = np.array([]), np.array([])\n",
        "\n",
        "        train_df = image_df[image_df['person_id'].isin(train_pids)].copy()\n",
        "        val_df = image_df[image_df['person_id'].isin(val_pids)].copy()\n",
        "        test_df = image_df[image_df['person_id'].isin(test_pids)].copy()\n",
        "\n",
        "        print(f\"Train set: {len(train_df)} samples from {len(train_pids)} patients.\")\n",
        "        print(f\"Validation set: {len(val_df)} samples from {len(val_pids)} patients.\")\n",
        "        print(f\"Test set: {len(test_df)} samples from {len(test_pids)} patients.\")\n",
        "\n",
        "        if len(train_pids)>0 and len(val_pids)>0: assert len(set(train_pids) & set(val_pids)) == 0, \"Patient overlap train/val!\"\n",
        "        if len(train_pids)>0 and len(test_pids)>0: assert len(set(train_pids) & set(test_pids)) == 0, \"Patient overlap train/test!\"\n",
        "        if len(val_pids)>0 and len(test_pids)>0: assert len(set(val_pids) & set(test_pids)) == 0, \"Patient overlap val/test!\"\n",
        "        print(\"Patient-level splits verified (no overlap if sets are non-empty).\")\n",
        "\n",
        "        if not train_df.empty and 'LDL' in train_df.columns:\n",
        "            print(\"\\nNormalizing LDL values using StandardScaler...\")\n",
        "            ldl_scaler = StandardScaler()\n",
        "            train_df['LDL_scaled'] = ldl_scaler.fit_transform(train_df[['LDL']])\n",
        "            if not val_df.empty: val_df['LDL_scaled'] = ldl_scaler.transform(val_df[['LDL']])\n",
        "            else: val_df['LDL_scaled'] = pd.Series(dtype='float64')\n",
        "            if not test_df.empty: test_df['LDL_scaled'] = ldl_scaler.transform(test_df[['LDL']])\n",
        "            else: test_df['LDL_scaled'] = pd.Series(dtype='float64')\n",
        "\n",
        "            print(\"LDL normalization complete.\")\n",
        "            print(\"Scaled LDL stats in train_df (should be mean~0, std~1):\")\n",
        "            from IPython.display import display\n",
        "            display(train_df['LDL_scaled'].describe())\n",
        "            # import joblib\n",
        "            # scaler_filename = 'ldl_scaler_unsloth.joblib'\n",
        "            # joblib.dump(ldl_scaler, scaler_filename)\n",
        "            # print(f\"LDL scaler saved to {scaler_filename}\")\n",
        "        else:\n",
        "            print(\"Train DataFrame is empty or 'LDL' column missing. Skipping LDL normalization.\")\n",
        "else:\n",
        "    print(\"image_df is empty. Skipping data splitting and LDL normalization.\")\n",
        "\n",
        "print(\"\\nCell 6: Data splitting and LDL normalization attempt complete.\")"
      ],
      "metadata": {
        "id": "WkxW5lpBt-7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Cell 5.1 (from user, now Cell 6.1): Check Unsloth tokenizer/model.processor\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n--- Sanity Check for Unsloth Components (Cell 6.1) ---\")\n",
        "if 'tokenizer' in locals() and tokenizer is not None:\n",
        "    print(f\"Unsloth tokenizer IS LOADED. Type: {type(tokenizer)}\")\n",
        "    if hasattr(tokenizer, 'image_processor') and tokenizer.image_processor is not None:\n",
        "        print(f\"  It has a tokenizer.image_processor of type: {type(tokenizer.image_processor)}\")\n",
        "    else:\n",
        "        print(\"  It does NOT have a direct `tokenizer.image_processor` attribute (or it's None).\")\n",
        "\n",
        "    if hasattr(model, 'processor') and model.processor is not None:\n",
        "        print(f\"Unsloth model.processor IS LOADED. Type: {type(model.processor)}\")\n",
        "        if hasattr(model.processor, 'image_processor') and model.processor.image_processor is not None:\n",
        "             print(f\"  model.processor has an image_processor component of type: {type(model.processor.image_processor)}\")\n",
        "    else:\n",
        "        print(\"  The model does NOT have a `model.processor` attribute (or it's None).\")\n",
        "\n",
        "    if not (hasattr(tokenizer, 'image_processor') and tokenizer.image_processor is not None) and \\\n",
        "       not (hasattr(model, 'processor') and model.processor is not None and hasattr(model.processor, 'image_processor')):\n",
        "        print(f\"  WARNING: No obvious image processor found. The model '{selected_model_name}' may be text-only.\")\n",
        "        print(\"  If your task requires image input, ensure you've selected a vision-language model and that Unsloth loads its image processor correctly.\")\n",
        "else:\n",
        "    print(\"Unsloth tokenizer IS NOT LOADED or is None.\")"
      ],
      "metadata": {
        "id": "ExnNlqz_uEpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HkmymzO7uLBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c19e4e06"
      },
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Cell 7: Custom PyTorch Dataset for DICOM Images and LDL (New Cell)\n",
        "# -----------------------------------------------------------------------------\n",
        "class DICOM_LDL_Dataset(Dataset):\n",
        "    def __init__(self, dataframe, image_root_dir, tokenizer, target_size=(896, 896)):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataframe (pd.DataFrame): DataFrame with 'image_path' and 'LDL_scaled' columns.\n",
        "            image_root_dir (str): Root directory where images are extracted. (Not strictly needed with full paths in df, but good practice)\n",
        "            tokenizer: The Unsloth tokenizer/processor with an image_processor component.\n",
        "            target_size (tuple): Desired image size (height, width).\n",
        "        \"\"\"\n",
        "        self.dataframe = dataframe\n",
        "        self.image_root_dir = image_root_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.target_size = target_size\n",
        "\n",
        "        if not hasattr(self.tokenizer, 'image_processor') or self.tokenizer.image_processor is None:\n",
        "             raise ValueError(\"Tokenizer must have a loaded 'image_processor' attribute.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_path = self.dataframe.iloc[idx]['image_path']\n",
        "        ldl_scaled = self.dataframe.iloc[idx]['LDL_scaled']\n",
        "\n",
        "        # --- Image Loading and Processing ---\n",
        "        try:\n",
        "            # Read DICOM file\n",
        "            dicom_data = pydicom.dcmread(img_path)\n",
        "            # Convert to NumPy array (handle different pixel data types and dimensions)\n",
        "            # Ensure pixel data is in standard format (e.g., uint8 or uint16)\n",
        "            if 'PixelData' not in dicom_data:\n",
        "                 raise ValueError(f\"No PixelData found in DICOM file: {img_path}\")\n",
        "\n",
        "            pixel_array = dicom_data.pixel_array\n",
        "\n",
        "            # Handle potential multi-frame DICOMs - take the first frame for simplicity\n",
        "            if pixel_array.ndim == 4: # (frames, height, width, channels)\n",
        "                image = pixel_array[0, :, :, :]\n",
        "            elif pixel_array.ndim == 3: # (height, width, channels) or (frames, height, width)\n",
        "                 if dicom_data.get(\"PhotometricInterpretation\", \"\").startswith(\"MONOCHROME\"):\n",
        "                      # Grayscale image (height, width) -> add channel dim\n",
        "                      image = pixel_array[:, :]\n",
        "                 else: # Assume (height, width, channels)\n",
        "                      image = pixel_array[:, :, :]\n",
        "            elif pixel_array.ndim == 2: # Grayscale image (height, width)\n",
        "                 image = pixel_array\n",
        "            else:\n",
        "                 raise ValueError(f\"Unsupported pixel array dimension: {pixel_array.ndim} for {img_path}\")\n",
        "\n",
        "            # Convert to PIL Image\n",
        "            # Need to handle different pixel formats and value ranges\n",
        "            # A common approach is to normalize or scale pixel values\n",
        "            # For simplicity, let's try scaling to 0-255 for common image formats\n",
        "            if image.dtype != np.uint8:\n",
        "                 # Simple scaling for visualization/processing compatibility\n",
        "                 image = image.astype(np.float32)\n",
        "                 image = (image - image.min()) / (image.max() - image.min()) * 255.0\n",
        "                 image = image.astype(np.uint8)\n",
        "\n",
        "            # Convert grayscale to RGB if needed by the model's image processor\n",
        "            if image.ndim == 2:\n",
        "                 image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "            elif image.ndim == 3 and image.shape[2] == 1: # (H, W, 1) grayscale\n",
        "                 image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "            elif image.ndim == 3 and image.shape[2] == 4: # (H, W, 4) RGBA\n",
        "                 image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
        "\n",
        "            image = Image.fromarray(image)\n",
        "\n",
        "            # Apply model's image processing (resizing, normalization etc.)\n",
        "            # The image_processor expects a PIL Image or a list of PIL Images\n",
        "            processed_images = self.tokenizer.image_processor(images=image, return_tensors=\"pt\")\n",
        "            # The output is typically a dictionary like {'pixel_values': tensor}\n",
        "            image_tensor = processed_images['pixel_values'].squeeze(0) # Remove batch dim added by processor\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {img_path}: {e}\")\n",
        "            # Handle errors: return None or a dummy sample, or raise error\n",
        "            # For now, let's raise to debug data loading issues\n",
        "            raise\n",
        "\n",
        "        # Convert LDL to tensor\n",
        "        ldl_tensor = torch.tensor(ldl_scaled, dtype=torch.float32)\n",
        "\n",
        "        # Return image tensor and LDL tensor\n",
        "        # The model might expect image input and text input separately.\n",
        "        # For now, we prepare the image tensor and the target LDL.\n",
        "        # The text part (\"predict LDL based on image\") will be handled during collation/training.\n",
        "        return {\n",
        "            \"pixel_values\": image_tensor,\n",
        "            \"labels\": ldl_tensor, # Using 'labels' as is standard in HF for supervised tasks\n",
        "            \"image_path\": img_path # Keep path for debugging\n",
        "        }\n",
        "\n",
        "print(\"\\nCell 7: Custom Dataset class defined.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cSq1JoFRvFZj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}