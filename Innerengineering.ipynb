{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOcTVJn5ayGZHFxtzeI7pZm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravikrishnan05/PrediscanMedtech_project/blob/main/Innerengineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skNz-5YwarRv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
        "import torch\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\", # Llama 3.2 vision support\n",
        "    \"unsloth/Llama-3.2-11B-Vision-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit\", # Can fit in a 80GB card!\n",
        "    \"unsloth/Llama-3.2-90B-Vision-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/Pixtral-12B-2409-bnb-4bit\",              # Pixtral fits in 16GB!\n",
        "    \"unsloth/Pixtral-12B-Base-2409-bnb-4bit\",         # Pixtral base model\n",
        "\n",
        "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",          # Qwen2 VL support\n",
        "    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit\",      # Any Llava variant works!\n",
        "    \"unsloth/llava-1.5-7b-hf-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, processor = FastVisionModel.from_pretrained(\n",
        "    \"google/medgemma-4b-pt\",\n",
        "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        ")"
      ],
      "metadata": {
        "id": "E89xb4aXa1fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Get Vision Feature Dimension\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- 1. Determining Vision Feature Dimension ---\")\n",
        "# Since we know MedGemma is a Gemma3-based model, we can rely on its config.\n",
        "if hasattr(model.config, 'vision_config') and hasattr(model.config.vision_config, 'hidden_size'):\n",
        "    vision_feature_dim = model.config.vision_config.hidden_size\n",
        "else:\n",
        "    # Fallback just in case, this is the known value for MedGemma 4B's SigLIP vision tower.\n",
        "    vision_feature_dim = 1024\n",
        "print(f\"Vision feature dimension is: {vision_feature_dim}\\n\")"
      ],
      "metadata": {
        "id": "zs2Ddo52bzbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install pydicom\n",
        "!pip install pydicom"
      ],
      "metadata": {
        "id": "eYMe9hf0gqgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Cell B: Architecture Definition, Verification, and Forward Pass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pydicom\n",
        "from PIL import Image\n",
        "\n",
        "# This cell assumes `model` and `processor` were loaded in the previous cell.\n",
        "# The `processor` object from FastVisionModel contains the image_processor and tokenizer.\n",
        "\n",
        "# 1. Get Vision Feature Dimension\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- 1. Determining Vision Feature Dimension ---\")\n",
        "# Since we know MedGemma is a Gemma3-based model, we can rely on its config.\n",
        "if hasattr(model.config, 'vision_config') and hasattr(model.config.vision_config, 'hidden_size'):\n",
        "    vision_feature_dim = model.config.vision_config.hidden_size\n",
        "else:\n",
        "    # Fallback just in case, this is the known value for MedGemma 4B's SigLIP vision tower.\n",
        "    vision_feature_dim = 1024\n",
        "print(f\"Vision feature dimension is: {vision_feature_dim}\\n\")\n",
        "\n",
        "\n",
        "# 2. Define the Vision Regressor Wrapper\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- 2. Defining the MedGemmaVisionRegressor Wrapper ---\")\n",
        "class MedGemmaVisionRegressor(nn.Module):\n",
        "    A wrapper to isolate the vision_tower and add a regression head.\n",
        "    def __init__(self, base_vlm_model, vision_feature_dim_input: int):\n",
        "        super().__init__()\n",
        "        self.base_vlm = base_vlm_model\n",
        "        # The model's operating dtype is now float32 due to the Unsloth fallback.\n",
        "        self.target_dtype = self.base_vlm.dtype\n",
        "        print(f\"[Regressor Init] Base model operating dtype: {self.target_dtype}\")\n",
        "\n",
        "        self.regression_head = nn.Sequential(\n",
        "            nn.Linear(vision_feature_dim_input, vision_feature_dim_input // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(vision_feature_dim_input // 2, 1)\n",
        "        ).to(dtype=self.target_dtype) # Cast head to the model's operating dtype (float32)\n",
        "\n",
        "    def forward(self, pixel_values: torch.Tensor, return_vision_features=False):\n",
        "        # Access the base model. FastVisionModel does not add a `.model` wrapper like PEFT.\n",
        "        base_model = self.base_vlm\n",
        "\n",
        "        # Process image through the Vision Tower (SigLIP Vision Transformer)\n",
        "        vision_outputs = base_model.vision_tower(pixel_values=pixel_values, return_dict=True)\n",
        "\n",
        "        # Extract the final pooled feature vector representing the image\n",
        "        image_features = vision_outputs.pooler_output\n",
        "\n",
        "        # Pass vision features through the Regression Head\n",
        "        ldl_prediction = self.regression_head(image_features)\n",
        "\n",
        "        if return_vision_features:\n",
        "            return ldl_prediction, image_features\n",
        "        else:\n",
        "            return ldl_prediction\n",
        "\n",
        "\n",
        "# 3. Instantiate the Full Model and Print Architecture\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- 3. Instantiating the Regressor and Printing Architecture ---\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "regressor_model = MedGemmaVisionRegressor(model, vision_feature_dim)\n",
        "regressor_model.to(device)\n",
        "\n",
        "print(\"\\n\\n=== MODEL ARCHITECTURE ===\")\n",
        "print(regressor_model)\n",
        "print(\"==========================\\n\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5EVrvrwmgKtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Cell B: Standalone Verification with User-Uploaded PNG/JPEG Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image # Pillow library for standard images\n",
        "import os\n",
        "from google.colab import files # Import the files module for uploading\n",
        "import numpy as np\n",
        "\n",
        "# This cell assumes `regressor_model`, `processor`, `device`, and `model` were created in the previous cell.\n",
        "\n",
        "# 1. Upload a Sample Image (PNG, JPG, etc.) from Your Computer\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- 1. Please Upload an Image (PNG, JPG, etc.) for Verification ---\")\n",
        "# This will open a file upload dialog in your browser.\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Check if a file was uploaded\n",
        "if not uploaded:\n",
        "    raise RuntimeError(\"No file was uploaded. Please run the cell again and select a file.\")\n",
        "\n",
        "# Get the filename of the uploaded file\n",
        "sample_image_filename = next(iter(uploaded))\n",
        "print(f\"\\nSuccessfully uploaded '{sample_image_filename}'.\")\n",
        "# The path to the uploaded file is just its filename in the current directory\n",
        "sample_image_path = sample_image_filename\n",
        "print(f\"Using uploaded image at path: ./{sample_image_path}\\n\")\n",
        "\n",
        "\n",
        "# 2. Pre-process the Uploaded Image\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- 2. Pre-processing the Image ---\")\n",
        "\n",
        "def preprocess_standard_image(image_path, processor_obj):\n",
        "    Loads a standard image (PNG, JPG) and uses the model's processor.\n",
        "\n",
        "    # Open the image using Pillow\n",
        "    try:\n",
        "        pil_image = Image.open(image_path)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to open image file '{image_path}'. Error: {e}\")\n",
        "\n",
        "    # Ensure the image is in RGB format, as most vision models expect 3 channels.\n",
        "    # This will convert grayscale or RGBA images correctly.\n",
        "    pil_image_rgb = pil_image.convert(\"RGB\")\n",
        "\n",
        "    # The `processor` object contains `image_processor` which handles everything else:\n",
        "    # resizing, normalization, and tensor conversion.\n",
        "    processed_output = processor_obj.image_processor(images=pil_image_rgb, return_tensors=\"pt\")\n",
        "\n",
        "    # Move the final tensor to the correct device and ensure its dtype matches the model.\n",
        "    # The model's operating dtype is likely float32 due to the Unsloth fallback on T4.\n",
        "    pixel_values = processed_output.pixel_values.to(device=device, dtype=model.dtype)\n",
        "\n",
        "    print(f\"Image processed into a tensor.\")\n",
        "    print(f\"Tensor Shape: {pixel_values.shape}\")\n",
        "    print(f\"Tensor Dtype: {pixel_values.dtype}\\n\")\n",
        "    return pixel_values\n",
        "\n",
        "# Prepare the input tensor from our uploaded sample\n",
        "image_tensor = preprocess_standard_image(sample_image_path, processor)\n",
        "\n",
        "\n",
        "# 3. Perform the Forward Pass and Inspect Outputs\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- 3. Performing Forward Pass and Inspecting Outputs ---\")\n",
        "regressor_model.eval() # Set model to evaluation mode\n",
        "\n",
        "with torch.no_grad(): # No need to track gradients for verification\n",
        "    # Call the model with our special flag to get intermediate features\n",
        "    final_prediction, vision_features = regressor_model(\n",
        "        image_tensor,\n",
        "        return_vision_features=True\n",
        "    )\n",
        "\n",
        "print(\"\\n=== VERIFICATION RESULTS ===\")\n",
        "# --- Output 1: After the Vision Transformer ---\n",
        "print(f\"\\n[OUTPUT 1] Intermediate Features from Vision Transformer (Vision Tower)\")\n",
        "print(f\"  - Shape: {vision_features.shape}\")\n",
        "print(f\"  - Dtype: {vision_features.dtype}\")\n",
        "print(f\"  - Device: {vision_features.device}\")\n",
        "print(f\"  - Sample Values (first 10 features): \\n{vision_features[0, :10].cpu().numpy()}\")\n",
        "print(\"  - This is the feature vector representing the image. Its shape should be (1, vision_feature_dim).\")\n",
        "\n",
        "# --- Output 2: After the Regression Head ---\n",
        "print(f\"\\n[OUTPUT 2] Final Numeric Prediction from Regression Head\")\n",
        "print(f\"  - Shape: {final_prediction.shape}\")\n",
        "print(f\"  - Dtype: {final_prediction.dtype}\")\n",
        "print(f\"  - Device: {final_prediction.device}\")\n",
        "print(f\"  - Predicted (Scaled) Value: {final_prediction.item():.4f}\")\n",
        "print(\"  - This is the final output, representing the predicted scaled LDL value.\")\n",
        "\n",
        "print(\"\\n==========================\")\n",
        "print(\"\\nVerification complete. The data flow from your uploaded image to a single number is working.\")\n",
        "print(\"WARNING: The model is running in float32, which may cause memory issues during full training.\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hHFgCihWgkY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Combined Cell: Architecture and Verification with Robust Feature Extraction\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "import os\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "try:\n",
        "    import pydicom\n",
        "    DICOM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    DICOM_AVAILABLE = False\n",
        "    print(\"pydicom library not found. DICOM file uploads will not work.\")\n",
        "\n",
        "# Assumes `model` and `processor` were loaded successfully in the first cell.\n",
        "\n",
        "# 1. Get Vision Feature Dimension\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- 1. Determining Vision Feature Dimension ---\")\n",
        "if hasattr(model.config, 'vision_config') and hasattr(model.config.vision_config, 'hidden_size'):\n",
        "    vision_feature_dim = model.config.vision_config.hidden_size\n",
        "else:\n",
        "    vision_feature_dim = 1152 # Fallback to the detected dimension\n",
        "print(f\"Vision feature dimension is: {vision_feature_dim}\\n\")\n",
        "\n",
        "\n",
        "# 2. Define the Vision Regressor Wrapper (with ROBUST forward pass)\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- 2. Defining the MedGemmaVisionRegressor Wrapper ---\")\n",
        "class MedGemmaVisionRegressor(nn.Module):\n",
        "    \"\"\"A wrapper with a robust forward pass for different Vision Transformer outputs.\"\"\"\n",
        "\n",
        "    #Here the MedGemmaVisionRegressor Class in PyTorch (inherits from nn.Module)which is the basic class\n",
        "\n",
        "    def __init__(self, base_vlm_model, vision_feature_dim_input: int): #Constructor of the child class .Here the define the architecture\n",
        "        super().__init__() #Calls the parent class (nn.Module)\n",
        "        #This line takes the massive, pre-trained MedGemma model (base_vlm_model)\n",
        "        #That we loaded with Unsloth and stores it as a part of our new object.\n",
        "\n",
        "        self.base_vlm = base_vlm_model\n",
        "        #self.base_vlm stores this model as an instance variable, so you can access it from other methods inside your class.\n",
        "\n",
        "        # Use the model's ACTUAL dtype, which is float32 because of the Unsloth fallback\n",
        "        self.target_dtype = self.base_vlm.dtype\n",
        "        # self.base_vlm.dtype checks the data type used by the base model’s parameters.this directly assigns It.\n",
        "        # This helps ensure that any new layers you add (like the regression head) will use the same dtype as the base model,\n",
        "        # avoiding errors like:\n",
        "        # Expected input to be of type torch.float32 but got torch.float16.\n",
        "        print(f\"[Regressor Init] Base model operating dtype: {self.target_dtype}\")\n",
        "\n",
        "        #The architecure defined here\n",
        "        self.regression_head = nn.Sequential(\n",
        "            nn.Linear(vision_feature_dim_input, vision_feature_dim_input // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(vision_feature_dim_input // 2, 1)\n",
        "        ).to(dtype=self.target_dtype)\n",
        "\n",
        "    #this is method overriding in pytorch nn.Module we already have an method were overriding it for our specific architecture\n",
        "    def forward(self, pixel_values: torch.Tensor, return_vision_features=False):\n",
        "        base_model = self.base_vlm\n",
        "\n",
        "        # --- ROBUST FEATURE EXTRACTION ---\n",
        "        # Call the vision tower\n",
        "        #we're giving ip to model.This returns a dictionary of outputs (e.g.,pooler_output, last_hidden_state, etc.)\n",
        "        vision_outputs = base_model.vision_tower(pixel_values=pixel_values, return_dict=True)\n",
        "        #Now vision_output is a dictonary with its keys as \"pooler_output\" ,\"last_hidden_state\"\n",
        "        #It looks like a dictionary, but it’s not always a regular Python dictionary — it’s often a custom object that acts like a dictionary.\n",
        "        #This behaves like a:\n",
        "        # Dictionary (you can use .get() or [\"key\"])\n",
        "        # Object (you can use .key like .last_hidden_state)\n",
        "        # So it's a hybrid.\n",
        "\n",
        "        # Try to get pooler_output first.\n",
        "        image_features = vision_outputs.get(\"pooler_output\") # Use .get() for safety\n",
        "        #.get() is a dictionary-safe way to fetch a key.O(1)\n",
        "\n",
        "        # If pooler_output is None (as we just discovered), fall back to last_hidden_state.\n",
        "        if image_features is None:\n",
        "            print(\"`pooler_output` is None. Falling back to using `last_hidden_state`.\")\n",
        "            # hasattr(obj, \"attr\") checks if the object has that attribute\n",
        "            if hasattr(vision_outputs, \"last_hidden_state\"):\n",
        "                # The shape is (batch_size, num_patches, hidden_size).\n",
        "                # We take the embedding of the first token ([CLS] token) as the image representation.\n",
        "                # gives one vecctor as were taking\n",
        "                image_features = vision_outputs.last_hidden_state[:, 0, :]\n",
        "                print(\"Shape of image_features (CLS token):\", image_features.shape)\n",
        "            else:\n",
        "                raise RuntimeError(\"The `vision_tower` output has neither `.pooler_output` nor `.last_hidden_state`. Cannot extract features.\")\n",
        "\n",
        "        # Pass features through the Regression Head\n",
        "        ldl_prediction = self.regression_head(image_features)\n",
        "\n",
        "        if return_vision_features:\n",
        "            return ldl_prediction, image_features\n",
        "        else:\n",
        "            return ldl_prediction\n",
        "\n",
        "\n",
        "# 3. Instantiate the Full Model\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- 3. Instantiating the Regressor ---\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "regressor_model = MedGemmaVisionRegressor(model, vision_feature_dim)\n",
        "regressor_model.to(device)\n",
        "print(\"Regressor model created and moved to device.\")\n",
        "\n",
        "\n",
        "# 4. Upload and Pre-process a Sample Image\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"\\n--- 4. Please Upload an Image (PNG, JPG, DCM, etc.) for Verification ---\")\n",
        "uploaded = files.upload()\n",
        "if not uploaded: raise RuntimeError(\"No file was uploaded.\")\n",
        "sample_image_filename = next(iter(uploaded))\n",
        "print(f\"\\nSuccessfully uploaded '{sample_image_filename}'.\\n\")\n",
        "\n",
        "def preprocess_any_image(image_path, processor_obj):\n",
        "    \"\"\"Loads a standard image or DICOM and uses the model's processor.\"\"\"\n",
        "    try:\n",
        "        pil_image = Image.open(image_path).convert(\"RGB\")\n",
        "        print(f\"Successfully loaded '{image_path}' as a standard image.\")\n",
        "    except Exception:\n",
        "        if DICOM_AVAILABLE:\n",
        "            print(f\"Could not open with Pillow, trying to load '{image_path}' as a DICOM file...\")\n",
        "            dicom_file = pydicom.dcmread(image_path)\n",
        "            pixel_array = dicom_file.pixel_array; pixel_array = ((pixel_array - np.min(pixel_array)) / (np.max(pixel_array) - np.min(pixel_array)) * 255.0).astype(np.uint8)\n",
        "            pil_image = Image.fromarray(pixel_array).convert('RGB')\n",
        "        else: raise RuntimeError(f\"Could not open '{image_path}' and pydicom is not available.\")\n",
        "\n",
        "    processed_output = processor_obj.image_processor(images=pil_image, return_tensors=\"pt\")\n",
        "    # VERY IMPORTANT: The log shows the model fell back to float32. We must match this.\n",
        "    pixel_values = processed_output.pixel_values.to(device=device, dtype=torch.float32)\n",
        "\n",
        "    print(f\"Image processed into a tensor.\")\n",
        "    print(f\"Tensor Shape: {pixel_values.shape}\")\n",
        "    print(f\"Tensor Dtype: {pixel_values.dtype}\\n\")\n",
        "    return pixel_values\n",
        "\n",
        "image_tensor = preprocess_any_image(sample_image_filename, processor)\n",
        "\n",
        "\n",
        "# 5. Perform the Forward Pass and Inspect Outputs\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- 5. Performing Forward Pass and Inspecting Outputs ---\")\n",
        "regressor_model.eval()\n",
        "with torch.no_grad():\n",
        "    final_prediction, vision_features = regressor_model(\n",
        "        image_tensor,\n",
        "        return_vision_features=True\n",
        "    )\n",
        "\n",
        "print(\"\\n=== VERIFICATION RESULTS ===\")\n",
        "print(f\"\\n[OUTPUT 1] Intermediate Features from Vision Transformer\")\n",
        "print(f\"  - Shape: {vision_features.shape}\")\n",
        "print(f\"  - Dtype: {vision_features.dtype}\")\n",
        "print(f\"  - Sample Values (first 10 features): \\n{vision_features[0, :10].cpu().numpy()}\")\n",
        "\n",
        "print(f\"\\n[OUTPUT 2] Final Numeric Prediction from Regression Head\")\n",
        "print(f\"  - Shape: {final_prediction.shape}\")\n",
        "print(f\"  - Dtype: {final_prediction.dtype}\")\n",
        "print(f\"  - Predicted (Scaled) Value: {final_prediction.item():.4f}\")\n",
        "\n",
        "print(\"\\n==========================\")\n",
        "print(\"\\nVerification complete. Data flow is now working correctly.\")"
      ],
      "metadata": {
        "id": "K71rd8eThgJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eVMbJkUpzVgu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77045b12"
      },
      "source": [
        "Explanation Breakdown\n",
        "1. The Constructor: __init__\n",
        "This method sets up a new \"instance\" or object of our class. It runs once when we create the model, like regressor_model = MedGemmaVisionRegressor(...).\n",
        "Technical Terms:\n",
        "class MedGemmaVisionRegressor(nn.Module): We declare a new class named MedGemmaVisionRegressor that inherits from torch.nn.Module. This inheritance gives our class all the powerful capabilities of a standard PyTorch model (like tracking parameters, moving to a GPU with .to(device), etc.).\n",
        "super().__init__(): This line is essential. It calls the constructor of the parent class (nn.Module) to properly initialize all the background machinery that PyTorch provides.\n",
        "Layman's Terms (Building a Custom Car):\n",
        "Think of nn.Module as a standard, high-quality car chassis that already has an engine mount, suspension, and electrical wiring.\n",
        "class MedGemmaVisionRegressor(...) is you declaring, \"I'm going to build a new, custom car model based on this standard chassis.\"\n",
        "super().__init__() is the first step where you take the chassis off the factory line. Now you're ready to add your custom parts.\n",
        "2. Storing Components as Attributes: self.base_vlm = base_vlm_model\n",
        "OOP Concept: Composition & Instance Attributes\n",
        "Our custom model is \"composed of\" other, pre-existing parts. This line takes the massive, pre-trained MedGemma model (base_vlm_model) that we loaded with Unsloth and stores it as a part of our new object.\n",
        "self.base_vlm becomes an instance attribute—a variable that belongs to this specific object. This makes the MedGemma model accessible from anywhere within our class.\n",
        "Layman's Terms (Installing the Engine):\n",
        "This is where you take a powerful, pre-built jet engine (the base_vlm_model) and bolt it onto your car chassis. It's now a permanent part of your custom car.\n",
        "3. Defining the Custom Regression Head\n",
        "This is where we build the new, small component that performs our specific task (predicting a single number).\n",
        "Technical Terms:\n",
        "self.regression_head: We create another instance attribute to hold our custom part.\n",
        "nn.Sequential: A PyTorch container that chains layers together in a sequence. The output of one layer automatically becomes the input to the next.\n",
        "nn.Linear(in_features, out_features): A standard fully-connected neural network layer. It performs a linear transformation (matrix multiplication) on the input.\n",
        "The first nn.Linear takes the image feature vector (e.g., of size 1152) and maps it to a smaller intermediate size.\n",
        "The second nn.Linear takes the intermediate vector and maps it to a single output (out_features=1), which is our final prediction.\n",
        "nn.ReLU(): A non-linear activation function. It's like a switch that adds complexity, allowing the model to learn more than just simple linear relationships.\n",
        "nn.Dropout(0.1): A regularization technique used during training to prevent overfitting. It randomly sets a fraction (10%) of input units to 0 at each update, forcing the network to learn more robust features.\n",
        "Layman's Terms (Building a Custom Dashboard):\n",
        "The jet engine (base_vlm) produces thousands of complex readings (the image_features). This is too much information for a simple speedometer.\n",
        "You build a custom dashboard (self.regression_head) to interpret these readings.\n",
        "The nn.Linear layers are like \"converters\" that process and simplify the data.\n",
        "The nn.ReLU is like a set of \"logic gates\" that help make sense of the signals.\n",
        "The final nn.Linear(..., 1) is the actual speedometer needle itself—it takes all the processed information and displays it as a single, final number.\n",
        "4. The Forward Pass: forward(...)\n",
        "This method defines the \"assembly line\"—the actual path data takes through the model when you make a prediction.\n",
        "OOP Concept: Methods\n",
        "A method defines an object's behavior. When you call regressor_model(image_tensor), PyTorch automatically executes this forward method.\n",
        "Layman's Terms (The Car's Operation Manual):\n",
        "This is the step-by-step guide for how your custom car works. It tells you what happens when you turn the key (i.e., provide an input).\n",
        "5. Accessing the Vision Tower: vision_outputs = base_model.vision_tower(...)\n",
        "Technical Terminology:\n",
        "base_model = self.base_vlm: We get our \"jet engine\" component.\n",
        "base_model.vision_tower: We access the specific part of the MedGemma model responsible for image processing. This vision_tower is the Vision Transformer (ViT).\n",
        "We call it like a function, passing the pixel_values (our prepared image tensor) to it.\n",
        "Layman's Analogy:\n",
        "Step 1 of the operation: The image (pixel_values) is sent to the jet engine's main computer (vision_tower). The computer analyzes the image and generates a detailed diagnostic report (vision_outputs).\n",
        "6. Extracting the Image Feature Vector\n",
        "This step unpacks the diagnostic report to get the one piece of information we need.\n",
        "Technical Terminology:\n",
        "vision_outputs.get(\"pooler_output\"): We first try to get the pooler_output, which is a pre-packaged summary vector.\n",
        "vision_outputs.last_hidden_state[:, 0, :]: If the summary isn't available (as was our case), we fall back to a more general method. We take all the detailed, step-by-step outputs (last_hidden_state) and select only the very first one ([:, 0, :]). This is the [CLS] token, which is conventionally used as the final summary of the entire image in many transformer models.\n",
        "Layman's Analogy:\n",
        "The diagnostic report (vision_outputs) is 500 pages long. We don't need all of it. We first look for the \"Executive Summary\" page (pooler_output).\n",
        "Since that page is missing, we go to the main report (last_hidden_state) and take the very first paragraph ([:, 0, :]), which we know is always the final conclusion. This paragraph is our image_features.\n",
        "7. The Connection: ldl_prediction = self.regression_head(image_features)\n",
        "This is the most critical connection point.\n",
        "Technical Terminology: The output from the previous step (image_features)—a tensor representing the image—is now passed as the input to our custom regression_head.\n",
        "Layman's Analogy:\n",
        "You take the final conclusion paragraph (image_features) from the engine's diagnostic report and hand it to your custom dashboard (regression_head). The dashboard reads this summary and moves its speedometer needle (ldl_prediction) to the correct position.\n",
        "8. Returning the Final Result: return ldl_prediction\n",
        "Technical Terminology: The forward method completes by returning the final tensor produced by the regression_head. This is the output of our entire custom model.\n",
        "Layman's Analogy:\n",
        "The car's operation is complete. The final reading on the speedometer (ldl_prediction) is the result."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- 3. Instantiating the Regressor and Printing Architecture ---\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "regressor_model = MedGemmaVisionRegressor(model, vision_feature_dim)\n",
        "regressor_model.to(device)\n",
        "\n",
        "print(\"\\n\\n=== MODEL ARCHITECTURE ===\")\n",
        "print(regressor_model)\n",
        "print(\"==========================\\n\")"
      ],
      "metadata": {
        "id": "ZGpIBBGz0Ukt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JE_U2noJ5-9a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}