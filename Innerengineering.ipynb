{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPat9K4xBAdNUJE4ZyGldPo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "26f78a861383433d8420531dad78f083": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09f2656fd84041e4a5a653cb39bbae7a",
              "IPY_MODEL_1925398c3e6048c0858c81bcdd0e26e0",
              "IPY_MODEL_e323bbc1287b4586bf5af8d7a3319158"
            ],
            "layout": "IPY_MODEL_71a4b870a73f4125ac7870ca2586a338"
          }
        },
        "09f2656fd84041e4a5a653cb39bbae7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d153ef798ec2494f8fc7ecf0698d48fc",
            "placeholder": "​",
            "style": "IPY_MODEL_b0e416c7db544eaa93017632ce89d478",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "1925398c3e6048c0858c81bcdd0e26e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_921d1296ea614aa0994a7fd847e3985e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c7edaaee12a4dd28f46f65e14316baf",
            "value": 2
          }
        },
        "e323bbc1287b4586bf5af8d7a3319158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ea164952bf444b39ffc2269400af602",
            "placeholder": "​",
            "style": "IPY_MODEL_ef8e1cdf4a4446e8bf20c1d6cd9a6bf8",
            "value": " 2/2 [00:50&lt;00:00, 24.71s/it]"
          }
        },
        "71a4b870a73f4125ac7870ca2586a338": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d153ef798ec2494f8fc7ecf0698d48fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0e416c7db544eaa93017632ce89d478": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "921d1296ea614aa0994a7fd847e3985e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c7edaaee12a4dd28f46f65e14316baf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ea164952bf444b39ffc2269400af602": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef8e1cdf4a4446e8bf20c1d6cd9a6bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravikrishnan05/PrediscanMedtech_project/blob/main/Innerengineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "skNz-5YwarRv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
        "import torch\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\", # Llama 3.2 vision support\n",
        "    \"unsloth/Llama-3.2-11B-Vision-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit\", # Can fit in a 80GB card!\n",
        "    \"unsloth/Llama-3.2-90B-Vision-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/Pixtral-12B-2409-bnb-4bit\",              # Pixtral fits in 16GB!\n",
        "    \"unsloth/Pixtral-12B-Base-2409-bnb-4bit\",         # Pixtral base model\n",
        "\n",
        "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",          # Qwen2 VL support\n",
        "    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit\",      # Any Llava variant works!\n",
        "    \"unsloth/llava-1.5-7b-hf-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, processor = FastVisionModel.from_pretrained(\n",
        "    \"google/medgemma-4b-pt\",\n",
        "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251,
          "referenced_widgets": [
            "26f78a861383433d8420531dad78f083",
            "09f2656fd84041e4a5a653cb39bbae7a",
            "1925398c3e6048c0858c81bcdd0e26e0",
            "e323bbc1287b4586bf5af8d7a3319158",
            "71a4b870a73f4125ac7870ca2586a338",
            "d153ef798ec2494f8fc7ecf0698d48fc",
            "b0e416c7db544eaa93017632ce89d478",
            "921d1296ea614aa0994a7fd847e3985e",
            "7c7edaaee12a4dd28f46f65e14316baf",
            "8ea164952bf444b39ffc2269400af602",
            "ef8e1cdf4a4446e8bf20c1d6cd9a6bf8"
          ]
        },
        "id": "E89xb4aXa1fN",
        "outputId": "a2f84ec0-8770-4667-c38d-38126518a7ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.7.3: Fast Gemma3 patching. Transformers: 4.53.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26f78a861383433d8420531dad78f083"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Get Vision Feature Dimension\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- 1. Determining Vision Feature Dimension ---\")\n",
        "# Since we know MedGemma is a Gemma3-based model, we can rely on its config.\n",
        "if hasattr(model.config, 'vision_config') and hasattr(model.config.vision_config, 'hidden_size'):\n",
        "    vision_feature_dim = model.config.vision_config.hidden_size\n",
        "else:\n",
        "    # Fallback just in case, this is the known value for MedGemma 4B's SigLIP vision tower.\n",
        "    vision_feature_dim = 1024\n",
        "print(f\"Vision feature dimension is: {vision_feature_dim}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs2Ddo52bzbk",
        "outputId": "2624cb48-df67-43c0-d370-68b0823978e3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Determining Vision Feature Dimension ---\n",
            "Vision feature dimension is: 1152\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install pydicom\n",
        "!pip install pydicom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYMe9hf0gqgn",
        "outputId": "33efaf59-8a6d-40e2-84cf-da35c0f2e66e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydicom\n",
            "  Downloading pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Cell B: Architecture Definition, Verification, and Forward Pass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pydicom\n",
        "from PIL import Image\n",
        "\n",
        "# This cell assumes `model` and `processor` were loaded in the previous cell.\n",
        "# The `processor` object from FastVisionModel contains the image_processor and tokenizer.\n",
        "\n",
        "# 1. Get Vision Feature Dimension\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- 1. Determining Vision Feature Dimension ---\")\n",
        "# Since we know MedGemma is a Gemma3-based model, we can rely on its config.\n",
        "if hasattr(model.config, 'vision_config') and hasattr(model.config.vision_config, 'hidden_size'):\n",
        "    vision_feature_dim = model.config.vision_config.hidden_size\n",
        "else:\n",
        "    # Fallback just in case, this is the known value for MedGemma 4B's SigLIP vision tower.\n",
        "    vision_feature_dim = 1024\n",
        "print(f\"Vision feature dimension is: {vision_feature_dim}\\n\")\n",
        "\n",
        "\n",
        "# 2. Define the Vision Regressor Wrapper\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- 2. Defining the MedGemmaVisionRegressor Wrapper ---\")\n",
        "class MedGemmaVisionRegressor(nn.Module):\n",
        "    \"\"\"A wrapper to isolate the vision_tower and add a regression head.\"\"\"\n",
        "    def __init__(self, base_vlm_model, vision_feature_dim_input: int):\n",
        "        super().__init__()\n",
        "        self.base_vlm = base_vlm_model\n",
        "        # The model's operating dtype is now float32 due to the Unsloth fallback.\n",
        "        self.target_dtype = self.base_vlm.dtype\n",
        "        print(f\"[Regressor Init] Base model operating dtype: {self.target_dtype}\")\n",
        "\n",
        "        self.regression_head = nn.Sequential(\n",
        "            nn.Linear(vision_feature_dim_input, vision_feature_dim_input // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(vision_feature_dim_input // 2, 1)\n",
        "        ).to(dtype=self.target_dtype) # Cast head to the model's operating dtype (float32)\n",
        "\n",
        "    def forward(self, pixel_values: torch.Tensor, return_vision_features=False):\n",
        "        # Access the base model. FastVisionModel does not add a `.model` wrapper like PEFT.\n",
        "        base_model = self.base_vlm\n",
        "\n",
        "        # Process image through the Vision Tower (SigLIP Vision Transformer)\n",
        "        vision_outputs = base_model.vision_tower(pixel_values=pixel_values, return_dict=True)\n",
        "\n",
        "        # Extract the final pooled feature vector representing the image\n",
        "        image_features = vision_outputs.pooler_output\n",
        "\n",
        "        # Pass vision features through the Regression Head\n",
        "        ldl_prediction = self.regression_head(image_features)\n",
        "\n",
        "        if return_vision_features:\n",
        "            return ldl_prediction, image_features\n",
        "        else:\n",
        "            return ldl_prediction\n",
        "\n",
        "\n",
        "# 3. Instantiate the Full Model and Print Architecture\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- 3. Instantiating the Regressor and Printing Architecture ---\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "regressor_model = MedGemmaVisionRegressor(model, vision_feature_dim)\n",
        "regressor_model.to(device)\n",
        "\n",
        "print(\"\\n\\n=== MODEL ARCHITECTURE ===\")\n",
        "print(regressor_model)\n",
        "print(\"==========================\\n\")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EVrvrwmgKtB",
        "outputId": "856cbdc3-c986-40d7-8e4a-0ab583815112"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Determining Vision Feature Dimension ---\n",
            "Vision feature dimension is: 1152\n",
            "\n",
            "--- 2. Defining the MedGemmaVisionRegressor Wrapper ---\n",
            "--- 3. Instantiating the Regressor and Printing Architecture ---\n",
            "[Regressor Init] Base model operating dtype: torch.float16\n",
            "\n",
            "\n",
            "=== MODEL ARCHITECTURE ===\n",
            "MedGemmaVisionRegressor(\n",
            "  (base_vlm): Gemma3ForConditionalGeneration(\n",
            "    (model): Gemma3Model(\n",
            "      (vision_tower): SiglipVisionModel(\n",
            "        (vision_model): SiglipVisionTransformer(\n",
            "          (embeddings): SiglipVisionEmbeddings(\n",
            "            (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
            "            (position_embedding): Embedding(4096, 1152)\n",
            "          )\n",
            "          (encoder): SiglipEncoder(\n",
            "            (layers): ModuleList(\n",
            "              (0-26): 27 x SiglipEncoderLayer(\n",
            "                (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
            "                (self_attn): SiglipAttention(\n",
            "                  (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
            "                  (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
            "                  (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
            "                  (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
            "                )\n",
            "                (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
            "                (mlp): SiglipMLP(\n",
            "                  (activation_fn): PytorchGELUTanh()\n",
            "                  (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
            "                  (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (multi_modal_projector): Gemma3MultiModalProjector(\n",
            "        (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
            "        (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
            "      )\n",
            "      (language_model): Gemma3TextModel(\n",
            "        (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
            "        (layers): ModuleList(\n",
            "          (0-33): 34 x Gemma3DecoderLayer(\n",
            "            (self_attn): Gemma3Attention(\n",
            "              (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
            "              (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
            "              (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
            "              (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
            "              (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
            "              (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
            "            )\n",
            "            (mlp): Gemma3MLP(\n",
            "              (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
            "              (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
            "              (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
            "              (act_fn): PytorchGELUTanh()\n",
            "            )\n",
            "            (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
            "            (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
            "            (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
            "            (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
            "          )\n",
            "        )\n",
            "        (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
            "        (rotary_emb): Gemma3RotaryEmbedding()\n",
            "        (rotary_emb_local): Gemma3RotaryEmbedding()\n",
            "      )\n",
            "    )\n",
            "    (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n",
            "  )\n",
            "  (regression_head): Sequential(\n",
            "    (0): Linear(in_features=1152, out_features=576, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.1, inplace=False)\n",
            "    (3): Linear(in_features=576, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "==========================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Cell B: Standalone Verification with User-Uploaded PNG/JPEG Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image # Pillow library for standard images\n",
        "import os\n",
        "from google.colab import files # Import the files module for uploading\n",
        "import numpy as np\n",
        "\n",
        "# This cell assumes `regressor_model`, `processor`, `device`, and `model` were created in the previous cell.\n",
        "\n",
        "# 1. Upload a Sample Image (PNG, JPG, etc.) from Your Computer\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- 1. Please Upload an Image (PNG, JPG, etc.) for Verification ---\")\n",
        "# This will open a file upload dialog in your browser.\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Check if a file was uploaded\n",
        "if not uploaded:\n",
        "    raise RuntimeError(\"No file was uploaded. Please run the cell again and select a file.\")\n",
        "\n",
        "# Get the filename of the uploaded file\n",
        "sample_image_filename = next(iter(uploaded))\n",
        "print(f\"\\nSuccessfully uploaded '{sample_image_filename}'.\")\n",
        "# The path to the uploaded file is just its filename in the current directory\n",
        "sample_image_path = sample_image_filename\n",
        "print(f\"Using uploaded image at path: ./{sample_image_path}\\n\")\n",
        "\n",
        "\n",
        "# 2. Pre-process the Uploaded Image\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- 2. Pre-processing the Image ---\")\n",
        "\n",
        "def preprocess_standard_image(image_path, processor_obj):\n",
        "    Loads a standard image (PNG, JPG) and uses the model's processor.\n",
        "\n",
        "    # Open the image using Pillow\n",
        "    try:\n",
        "        pil_image = Image.open(image_path)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to open image file '{image_path}'. Error: {e}\")\n",
        "\n",
        "    # Ensure the image is in RGB format, as most vision models expect 3 channels.\n",
        "    # This will convert grayscale or RGBA images correctly.\n",
        "    pil_image_rgb = pil_image.convert(\"RGB\")\n",
        "\n",
        "    # The `processor` object contains `image_processor` which handles everything else:\n",
        "    # resizing, normalization, and tensor conversion.\n",
        "    processed_output = processor_obj.image_processor(images=pil_image_rgb, return_tensors=\"pt\")\n",
        "\n",
        "    # Move the final tensor to the correct device and ensure its dtype matches the model.\n",
        "    # The model's operating dtype is likely float32 due to the Unsloth fallback on T4.\n",
        "    pixel_values = processed_output.pixel_values.to(device=device, dtype=model.dtype)\n",
        "\n",
        "    print(f\"Image processed into a tensor.\")\n",
        "    print(f\"Tensor Shape: {pixel_values.shape}\")\n",
        "    print(f\"Tensor Dtype: {pixel_values.dtype}\\n\")\n",
        "    return pixel_values\n",
        "\n",
        "# Prepare the input tensor from our uploaded sample\n",
        "image_tensor = preprocess_standard_image(sample_image_path, processor)\n",
        "\n",
        "\n",
        "# 3. Perform the Forward Pass and Inspect Outputs\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- 3. Performing Forward Pass and Inspecting Outputs ---\")\n",
        "regressor_model.eval() # Set model to evaluation mode\n",
        "\n",
        "with torch.no_grad(): # No need to track gradients for verification\n",
        "    # Call the model with our special flag to get intermediate features\n",
        "    final_prediction, vision_features = regressor_model(\n",
        "        image_tensor,\n",
        "        return_vision_features=True\n",
        "    )\n",
        "\n",
        "print(\"\\n=== VERIFICATION RESULTS ===\")\n",
        "# --- Output 1: After the Vision Transformer ---\n",
        "print(f\"\\n[OUTPUT 1] Intermediate Features from Vision Transformer (Vision Tower)\")\n",
        "print(f\"  - Shape: {vision_features.shape}\")\n",
        "print(f\"  - Dtype: {vision_features.dtype}\")\n",
        "print(f\"  - Device: {vision_features.device}\")\n",
        "print(f\"  - Sample Values (first 10 features): \\n{vision_features[0, :10].cpu().numpy()}\")\n",
        "print(\"  - This is the feature vector representing the image. Its shape should be (1, vision_feature_dim).\")\n",
        "\n",
        "# --- Output 2: After the Regression Head ---\n",
        "print(f\"\\n[OUTPUT 2] Final Numeric Prediction from Regression Head\")\n",
        "print(f\"  - Shape: {final_prediction.shape}\")\n",
        "print(f\"  - Dtype: {final_prediction.dtype}\")\n",
        "print(f\"  - Device: {final_prediction.device}\")\n",
        "print(f\"  - Predicted (Scaled) Value: {final_prediction.item():.4f}\")\n",
        "print(\"  - This is the final output, representing the predicted scaled LDL value.\")\n",
        "\n",
        "print(\"\\n==========================\")\n",
        "print(\"\\nVerification complete. The data flow from your uploaded image to a single number is working.\")\n",
        "print(\"WARNING: The model is running in float32, which may cause memory issues during full training.\")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "hHFgCihWgkY5",
        "outputId": "3fb16230-7968-489f-a5e5-77d187845976"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Cell B: Standalone Verification with User-Uploaded PNG/JPEG Image\\n\\nimport torch\\nimport torch.nn as nn\\nfrom PIL import Image # Pillow library for standard images\\nimport os\\nfrom google.colab import files # Import the files module for uploading\\nimport numpy as np\\n\\n# This cell assumes `regressor_model`, `processor`, `device`, and `model` were created in the previous cell.\\n\\n# 1. Upload a Sample Image (PNG, JPG, etc.) from Your Computer\\n# ----------------------------------------------------------------------------\\nprint(\"--- 1. Please Upload an Image (PNG, JPG, etc.) for Verification ---\")\\n# This will open a file upload dialog in your browser.\\nuploaded = files.upload()\\n\\n# Check if a file was uploaded\\nif not uploaded:\\n    raise RuntimeError(\"No file was uploaded. Please run the cell again and select a file.\")\\n\\n# Get the filename of the uploaded file\\nsample_image_filename = next(iter(uploaded))\\nprint(f\"\\nSuccessfully uploaded \\'{sample_image_filename}\\'.\")\\n# The path to the uploaded file is just its filename in the current directory\\nsample_image_path = sample_image_filename\\nprint(f\"Using uploaded image at path: ./{sample_image_path}\\n\")\\n\\n\\n# 2. Pre-process the Uploaded Image\\n# ----------------------------------------------------------------------------\\nprint(\"--- 2. Pre-processing the Image ---\")\\n\\ndef preprocess_standard_image(image_path, processor_obj):\\n    Loads a standard image (PNG, JPG) and uses the model\\'s processor.\\n    \\n    # Open the image using Pillow\\n    try:\\n        pil_image = Image.open(image_path)\\n    except Exception as e:\\n        raise RuntimeError(f\"Failed to open image file \\'{image_path}\\'. Error: {e}\")\\n\\n    # Ensure the image is in RGB format, as most vision models expect 3 channels.\\n    # This will convert grayscale or RGBA images correctly.\\n    pil_image_rgb = pil_image.convert(\"RGB\")\\n    \\n    # The `processor` object contains `image_processor` which handles everything else:\\n    # resizing, normalization, and tensor conversion.\\n    processed_output = processor_obj.image_processor(images=pil_image_rgb, return_tensors=\"pt\")\\n    \\n    # Move the final tensor to the correct device and ensure its dtype matches the model.\\n    # The model\\'s operating dtype is likely float32 due to the Unsloth fallback on T4.\\n    pixel_values = processed_output.pixel_values.to(device=device, dtype=model.dtype)\\n    \\n    print(f\"Image processed into a tensor.\")\\n    print(f\"Tensor Shape: {pixel_values.shape}\")\\n    print(f\"Tensor Dtype: {pixel_values.dtype}\\n\")\\n    return pixel_values\\n\\n# Prepare the input tensor from our uploaded sample\\nimage_tensor = preprocess_standard_image(sample_image_path, processor)\\n\\n\\n# 3. Perform the Forward Pass and Inspect Outputs\\n# ----------------------------------------------------------------------------\\nprint(\"--- 3. Performing Forward Pass and Inspecting Outputs ---\")\\nregressor_model.eval() # Set model to evaluation mode\\n\\nwith torch.no_grad(): # No need to track gradients for verification\\n    # Call the model with our special flag to get intermediate features\\n    final_prediction, vision_features = regressor_model(\\n        image_tensor,\\n        return_vision_features=True\\n    )\\n\\nprint(\"\\n=== VERIFICATION RESULTS ===\")\\n# --- Output 1: After the Vision Transformer ---\\nprint(f\"\\n[OUTPUT 1] Intermediate Features from Vision Transformer (Vision Tower)\")\\nprint(f\"  - Shape: {vision_features.shape}\")\\nprint(f\"  - Dtype: {vision_features.dtype}\")\\nprint(f\"  - Device: {vision_features.device}\")\\nprint(f\"  - Sample Values (first 10 features): \\n{vision_features[0, :10].cpu().numpy()}\")\\nprint(\"  - This is the feature vector representing the image. Its shape should be (1, vision_feature_dim).\")\\n\\n# --- Output 2: After the Regression Head ---\\nprint(f\"\\n[OUTPUT 2] Final Numeric Prediction from Regression Head\")\\nprint(f\"  - Shape: {final_prediction.shape}\")\\nprint(f\"  - Dtype: {final_prediction.dtype}\")\\nprint(f\"  - Device: {final_prediction.device}\")\\nprint(f\"  - Predicted (Scaled) Value: {final_prediction.item():.4f}\")\\nprint(\"  - This is the final output, representing the predicted scaled LDL value.\")\\n\\nprint(\"\\n==========================\")\\nprint(\"\\nVerification complete. The data flow from your uploaded image to a single number is working.\")\\nprint(\"WARNING: The model is running in float32, which may cause memory issues during full training.\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Combined Cell: Architecture and Verification with Robust Feature Extraction\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "import os\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "try:\n",
        "    import pydicom\n",
        "    DICOM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    DICOM_AVAILABLE = False\n",
        "    print(\"pydicom library not found. DICOM file uploads will not work.\")\n",
        "\n",
        "# Assumes `model` and `processor` were loaded successfully in the first cell.\n",
        "\n",
        "# 1. Get Vision Feature Dimension\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- 1. Determining Vision Feature Dimension ---\")\n",
        "if hasattr(model.config, 'vision_config') and hasattr(model.config.vision_config, 'hidden_size'):\n",
        "    vision_feature_dim = model.config.vision_config.hidden_size\n",
        "else:\n",
        "    vision_feature_dim = 1152 # Fallback to the detected dimension\n",
        "print(f\"Vision feature dimension is: {vision_feature_dim}\\n\")\n",
        "\n",
        "\n",
        "# 2. Define the Vision Regressor Wrapper (with ROBUST forward pass)\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- 2. Defining the MedGemmaVisionRegressor Wrapper ---\")\n",
        "class MedGemmaVisionRegressor(nn.Module):\n",
        "    \"\"\"A wrapper with a robust forward pass for different Vision Transformer outputs.\"\"\"\n",
        "    def __init__(self, base_vlm_model, vision_feature_dim_input: int):\n",
        "        super().__init__()\n",
        "        self.base_vlm = base_vlm_model\n",
        "        # Use the model's ACTUAL dtype, which is float32 because of the Unsloth fallback\n",
        "        self.target_dtype = self.base_vlm.dtype\n",
        "        print(f\"[Regressor Init] Base model operating dtype: {self.target_dtype}\")\n",
        "\n",
        "        self.regression_head = nn.Sequential(\n",
        "            nn.Linear(vision_feature_dim_input, vision_feature_dim_input // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(vision_feature_dim_input // 2, 1)\n",
        "        ).to(dtype=self.target_dtype)\n",
        "\n",
        "    def forward(self, pixel_values: torch.Tensor, return_vision_features=False):\n",
        "        base_model = self.base_vlm\n",
        "\n",
        "        # --- ROBUST FEATURE EXTRACTION ---\n",
        "        # Call the vision tower\n",
        "        vision_outputs = base_model.vision_tower(pixel_values=pixel_values, return_dict=True)\n",
        "\n",
        "        # Try to get pooler_output first.\n",
        "        image_features = vision_outputs.get(\"pooler_output\") # Use .get() for safety\n",
        "\n",
        "        # If pooler_output is None (as we just discovered), fall back to last_hidden_state.\n",
        "        if image_features is None:\n",
        "            print(\"`pooler_output` is None. Falling back to using `last_hidden_state`.\")\n",
        "            if hasattr(vision_outputs, \"last_hidden_state\"):\n",
        "                # The shape is (batch_size, num_patches, hidden_size).\n",
        "                # We take the embedding of the first token ([CLS] token) as the image representation.\n",
        "                image_features = vision_outputs.last_hidden_state[:, 0, :]\n",
        "            else:\n",
        "                raise RuntimeError(\"The `vision_tower` output has neither `.pooler_output` nor `.last_hidden_state`. Cannot extract features.\")\n",
        "\n",
        "        # Pass features through the Regression Head\n",
        "        ldl_prediction = self.regression_head(image_features)\n",
        "\n",
        "        if return_vision_features:\n",
        "            return ldl_prediction, image_features\n",
        "        else:\n",
        "            return ldl_prediction\n",
        "\n",
        "\n",
        "# 3. Instantiate the Full Model\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- 3. Instantiating the Regressor ---\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "regressor_model = MedGemmaVisionRegressor(model, vision_feature_dim)\n",
        "regressor_model.to(device)\n",
        "print(\"Regressor model created and moved to device.\")\n",
        "\n",
        "\n",
        "# 4. Upload and Pre-process a Sample Image\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"\\n--- 4. Please Upload an Image (PNG, JPG, DCM, etc.) for Verification ---\")\n",
        "uploaded = files.upload()\n",
        "if not uploaded: raise RuntimeError(\"No file was uploaded.\")\n",
        "sample_image_filename = next(iter(uploaded))\n",
        "print(f\"\\nSuccessfully uploaded '{sample_image_filename}'.\\n\")\n",
        "\n",
        "def preprocess_any_image(image_path, processor_obj):\n",
        "    \"\"\"Loads a standard image or DICOM and uses the model's processor.\"\"\"\n",
        "    try:\n",
        "        pil_image = Image.open(image_path).convert(\"RGB\")\n",
        "        print(f\"Successfully loaded '{image_path}' as a standard image.\")\n",
        "    except Exception:\n",
        "        if DICOM_AVAILABLE:\n",
        "            print(f\"Could not open with Pillow, trying to load '{image_path}' as a DICOM file...\")\n",
        "            dicom_file = pydicom.dcmread(image_path)\n",
        "            pixel_array = dicom_file.pixel_array; pixel_array = ((pixel_array - np.min(pixel_array)) / (np.max(pixel_array) - np.min(pixel_array)) * 255.0).astype(np.uint8)\n",
        "            pil_image = Image.fromarray(pixel_array).convert('RGB')\n",
        "        else: raise RuntimeError(f\"Could not open '{image_path}' and pydicom is not available.\")\n",
        "\n",
        "    processed_output = processor_obj.image_processor(images=pil_image, return_tensors=\"pt\")\n",
        "    # VERY IMPORTANT: The log shows the model fell back to float32. We must match this.\n",
        "    pixel_values = processed_output.pixel_values.to(device=device, dtype=torch.float32)\n",
        "\n",
        "    print(f\"Image processed into a tensor.\")\n",
        "    print(f\"Tensor Shape: {pixel_values.shape}\")\n",
        "    print(f\"Tensor Dtype: {pixel_values.dtype}\\n\")\n",
        "    return pixel_values\n",
        "\n",
        "image_tensor = preprocess_any_image(sample_image_filename, processor)\n",
        "\n",
        "\n",
        "# 5. Perform the Forward Pass and Inspect Outputs\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- 5. Performing Forward Pass and Inspecting Outputs ---\")\n",
        "regressor_model.eval()\n",
        "with torch.no_grad():\n",
        "    final_prediction, vision_features = regressor_model(\n",
        "        image_tensor,\n",
        "        return_vision_features=True\n",
        "    )\n",
        "\n",
        "print(\"\\n=== VERIFICATION RESULTS ===\")\n",
        "print(f\"\\n[OUTPUT 1] Intermediate Features from Vision Transformer\")\n",
        "print(f\"  - Shape: {vision_features.shape}\")\n",
        "print(f\"  - Dtype: {vision_features.dtype}\")\n",
        "print(f\"  - Sample Values (first 10 features): \\n{vision_features[0, :10].cpu().numpy()}\")\n",
        "\n",
        "print(f\"\\n[OUTPUT 2] Final Numeric Prediction from Regression Head\")\n",
        "print(f\"  - Shape: {final_prediction.shape}\")\n",
        "print(f\"  - Dtype: {final_prediction.dtype}\")\n",
        "print(f\"  - Predicted (Scaled) Value: {final_prediction.item():.4f}\")\n",
        "\n",
        "print(\"\\n==========================\")\n",
        "print(\"\\nVerification complete. Data flow is now working correctly.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "id": "K71rd8eThgJw",
        "outputId": "6bf0af63-13fb-48f0-ef11-25607c35f88b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Determining Vision Feature Dimension ---\n",
            "Vision feature dimension is: 1152\n",
            "\n",
            "--- 2. Defining the MedGemmaVisionRegressor Wrapper ---\n",
            "--- 3. Instantiating the Regressor ---\n",
            "[Regressor Init] Base model operating dtype: torch.float16\n",
            "Regressor model created and moved to device.\n",
            "\n",
            "--- 4. Please Upload an Image (PNG, JPG, DCM, etc.) for Verification ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6583939b-8275-484a-a378-45fa49e3429f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6583939b-8275-484a-a378-45fa49e3429f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1.png to 1 (3).png\n",
            "\n",
            "Successfully uploaded '1 (3).png'.\n",
            "\n",
            "Successfully loaded '1 (3).png' as a standard image.\n",
            "Image processed into a tensor.\n",
            "Tensor Shape: torch.Size([1, 3, 896, 896])\n",
            "Tensor Dtype: torch.float32\n",
            "\n",
            "--- 5. Performing Forward Pass and Inspecting Outputs ---\n",
            "`pooler_output` is None. Falling back to using `last_hidden_state`.\n",
            "\n",
            "=== VERIFICATION RESULTS ===\n",
            "\n",
            "[OUTPUT 1] Intermediate Features from Vision Transformer\n",
            "  - Shape: torch.Size([1, 1152])\n",
            "  - Dtype: torch.float16\n",
            "  - Sample Values (first 10 features): \n",
            "[ 0.8525  -0.1282   1.039   -0.207   -0.337    0.2384   0.1703  -0.4177\n",
            "  0.04437  0.1947 ]\n",
            "\n",
            "[OUTPUT 2] Final Numeric Prediction from Regression Head\n",
            "  - Shape: torch.Size([1, 1])\n",
            "  - Dtype: torch.float16\n",
            "  - Predicted (Scaled) Value: 0.4641\n",
            "\n",
            "==========================\n",
            "\n",
            "Verification complete. Data flow is now working correctly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wC6xW15Drk__"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}