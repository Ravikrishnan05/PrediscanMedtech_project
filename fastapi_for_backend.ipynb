{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEvMaqvHGcdU5oLGo53msq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravikrishnan05/PrediscanMedtech_project/blob/main/fastapi_for_backend.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MqlQGidqkl2"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "# RUN THIS CELL FIRST to forcefully uninstall the entire conflicting PyTorch ecosystem.\n",
        "!pip uninstall -y torch torchvision torchaudio triton flash-attn unsloth bitsandbytes xformers fastai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# NOW, RUN THE OFFICIAL INSTALLATION.\n",
        "# It will install torch, triton, bitsandbytes, and flash-attn correctly.\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "id": "BrrsObcOqptC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# After Unsloth is installed, we can install compatible versions of these libraries.\n",
        "!pip install torchvision torchaudio"
      ],
      "metadata": {
        "id": "KELkblbDqr5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# AFTER RESTARTING, RUN THIS CELL DIRECTLY\n",
        "\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "# Verification\n",
        "import unsloth\n",
        "print(\"âœ… Unsloth and its dependencies are loaded correctly!\")\n",
        "print(\"Unsloth version:\", unsloth.__version__)\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "if torch.cuda.is_available():\n",
        "    # Let's also check torchvision, which we will install\n",
        "    try:\n",
        "        import torchvision\n",
        "        print(\"Torchvision version:\", torchvision.__version__)\n",
        "    except ImportError:\n",
        "        print(\"Installing compatible torchvision...\")\n",
        "        !pip install torchvision torchaudio\n",
        "        import torchvision\n",
        "        print(\"Torchvision version:\", torchvision.__version__)\n",
        "\n",
        "\n",
        "# --- Model Initialization ---\n",
        "model_id = \"google/medgemma-4b-it\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_id,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "print(\"\\nâœ… Model and processor loaded successfully!\")"
      ],
      "metadata": {
        "id": "MGZKNHcFquwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Install the necessary packages for running the web API\n",
        "!pip install fastapi uvicorn \"python-multipart<0.0.7\" pyngrok nest_asyncio"
      ],
      "metadata": {
        "id": "qhqhS7U3tQ-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# RUN THIS CELL TO START YOUR API SERVER\n",
        "# %%\n",
        "# --- REFINED AND MORE ROBUST API SERVER ---\n",
        "\n",
        "# (Imports and app setup are the same)\n",
        "from fastapi import FastAPI, File, UploadFile, Form\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from PIL import Image\n",
        "import torch\n",
        "import io\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import traceback # Import for detailed error logging\n",
        "\n",
        "app = FastAPI()\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"], allow_credentials=True, allow_methods=[\"*\"], allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    return {\"message\": \"âœ… MedGemma API is working!\"}\n",
        "\n",
        "@app.post(\"/generate\")\n",
        "async def generate(prompt: str = Form(...), image: UploadFile = File(...)):\n",
        "    try:\n",
        "        image_bytes = await image.read()\n",
        "        pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "\n",
        "        # The standard way to format input for MedGemma / Idefics2 models\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\"},\n",
        "                    {\"type\": \"text\", \"text\": prompt},\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # The processor handles text and images together\n",
        "        prompt_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "        inputs = processor(text=prompt_text, images=pil_image, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # Generate\n",
        "        generated_ids = model.generate(**inputs, max_new_tokens=512, pad_token_id=tokenizer.pad_token_id)\n",
        "        generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "        # The response is typically the last part of the generated text\n",
        "        response_text = generated_texts[0].split(\"Assistant: \")[-1].strip()\n",
        "\n",
        "        return JSONResponse(content={\"response\": response_text})\n",
        "\n",
        "    except Exception as e:\n",
        "        # This will print the full error to your server log for easy debugging\n",
        "        print(traceback.format_exc())\n",
        "        return JSONResponse(status_code=500, content={\"error\": str(e)})"
      ],
      "metadata": {
        "id": "EDaqyfGgwByQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Launch the Server ---\n",
        "# Set your ngrok auth token\n",
        "ngrok.set_auth_token(\"2yH8AJ7M9rJNbLfQoRANIuEudom_5Ppznh7REV7PvB51AaZXe\")\n",
        "\n",
        "# Allow uvicorn to run in a notebook\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Connect to ngrok and get a public URL\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"ðŸŒ Public API URL:\", public_url)\n",
        "\n",
        "# Start the uvicorn server\n",
        "uvicorn.run(app, port=8000)"
      ],
      "metadata": {
        "id": "vXADC7qXwLPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# --- LOCALHOST TEST SCRIPT ---\n",
        "# This test bypasses ngrok and the internet completely.\n",
        "\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# We connect directly to the server inside the Colab machine\n",
        "local_api_url = \"http://127.0.0.1:8000\"\n",
        "\n",
        "image_path = \"/content/1.png\"\n",
        "\n",
        "if not os.path.exists(image_path):\n",
        "    print(f\"âŒ Error: Test image '{image_path}' not found. Please upload it.\")\n",
        "else:\n",
        "    endpoint = f\"{local_api_url}/generate\"\n",
        "    data = {\"prompt\": \"What are the key findings in this fundus image\"}\n",
        "    files = {\"image\": (image_path, open(image_path, \"rb\"), \"image/png\")}\n",
        "\n",
        "    print(\"ðŸš€ Sending request directly to localhost...\")\n",
        "    try:\n",
        "        # We send the request to the local server\n",
        "        response = requests.post(endpoint, data=data, files=files, timeout=300) # 5 min timeout\n",
        "        response.raise_for_status()\n",
        "        print(\"\\nâœ… Success! Localhost API Response:\")\n",
        "        print(response.json())\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"\\nâŒ An error occurred during the local request: {e}\")"
      ],
      "metadata": {
        "id": "cnQp-onf0NwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YiVgFaJ11i__"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}